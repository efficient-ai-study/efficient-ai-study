[
    {
        "client_msg_id": "0a49456a-3d2d-46d7-bccd-312376db5b3d",
        "type": "message",
        "text": "Transformer 모델의 속도를 높이기 위해, 덜 중요한 token들을 제거해 나가면서 추론하는 방법(a.k.a. token pruning)이 활발히 연구되고 있습니다. 이 중 이번 ICLR에 발표될 논문을 하나 가져왔습니다:\n<https:\/\/openreview.net\/forum?id=VV0hSE8AxCw|Sparse Token Transformers with Attention Back Tracking (ICLR 2023)>\n• 기존 방법: 이전 층에서 attention weight가 작았던 token들을 제거하고 남은 token들로 다음 층 연산을 수행합니다. \n    ◦ 이런 접근 방식은 최종 output에 미칠 영향을 전체적으로 고려하지 못한다는 단점이 있고, 몇 개나\/어떤 기준으로 제거해야 하는지가 연구의 중요한 부분이었습니다.\n    ◦ <https:\/\/arxiv.org\/abs\/2001.08950|PoWER-BERT>, <https:\/\/arxiv.org\/abs\/2010.07003|Length-Adaptive Transformer>, <https:\/\/arxiv.org\/abs\/2012.09852|SpAtten>, <https:\/\/arxiv.org\/abs\/2107.00910|Learned Token Pruning>, <https:\/\/arxiv.org\/abs\/2210.05832|SaiT>\n• 핵심 아이디어: 모든 층의 attention weight를 예측하는 별도의 작은 모델을 훈련하고, 그렇게 찾은 attention weight들을 사용해 최종 token [CLS]에 영향을 크게 미치는 정도를 위쪽 층부터 아래쪽 층까지 back-tracking합니다.\n    ◦ 이 논문은 이미 pruning 없이 훈련된 모델에 대해, 모델을 추가로 훈련하지는 않고 inference 속도만 높이는 것이 목적입니다.\n    ◦ 위에서 언급한 '작은 모델'은 이미 있는 본 모델의 attention weight를 따라가도록 knowledge distillation으로 훈련됩니다.\n    ◦ CV(image classification)에서 20%  이하, NLP(GLUE)에서 10% 이하의 token들만 사용해도 성능이 크게 안 떨어진다고 합니다.\n• 의견: 실제로 얼마나 빨라지는지는 별개로, 기존 방법의 문제점을 잘 다룬 것 같습니다.\n    ◦ 모든 token에서 output을 내 놔야 하는 문제에 대해서는 token pruning이 깔끔한 방책을 못 내고 있는 것 같습니다. 위의 Length-Adaptive Transformer가 그나마 해결책을 내긴 합니다.\n    ◦ GPT와 같은 auto-regressive Transformer에서도 이런 token purning이 사용될 수 있을지 보는 것도 흥미로운 주제라고 생각합니다.\n    ◦ 최종 output에 각 token이 얼마나 기여하는지를 알아내고자 한다는 점에서, 목적은 다르지만 attention map visualization과도 접점이 있을 것 같습니다.",
        "user": "U04MQ2J38JC",
        "ts": "1676212879.655989",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "3BpE",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "Transformer 모델의 속도를 높이기 위해, 덜 중요한 token들을 제거해 나가면서 추론하는 방법(a.k.a. token pruning)이 활발히 연구되고 있습니다. 이 중 이번 ICLR에 발표될 논문을 하나 가져왔습니다:\n"
                            },
                            {
                                "type": "link",
                                "url": "https:\/\/openreview.net\/forum?id=VV0hSE8AxCw",
                                "text": "Sparse Token Transformers with Attention Back Tracking (ICLR 2023)"
                            },
                            {
                                "type": "text",
                                "text": "\n"
                            }
                        ]
                    },
                    {
                        "type": "rich_text_list",
                        "elements": [
                            {
                                "type": "rich_text_section",
                                "elements": [
                                    {
                                        "type": "text",
                                        "text": "기존 방법: 이전 층에서 attention weight가 작았던 token들을 제거하고 남은 token들로 다음 층 연산을 수행합니다. "
                                    }
                                ]
                            }
                        ],
                        "style": "bullet",
                        "indent": 0,
                        "border": 0
                    },
                    {
                        "type": "rich_text_list",
                        "elements": [
                            {
                                "type": "rich_text_section",
                                "elements": [
                                    {
                                        "type": "text",
                                        "text": "이런 접근 방식은 최종 output에 미칠 영향을 전체적으로 고려하지 못한다는 단점이 있고, 몇 개나\/어떤 기준으로 제거해야 하는지가 연구의 중요한 부분이었습니다."
                                    }
                                ]
                            },
                            {
                                "type": "rich_text_section",
                                "elements": [
                                    {
                                        "type": "link",
                                        "url": "https:\/\/arxiv.org\/abs\/2001.08950",
                                        "text": "PoWER-BERT"
                                    },
                                    {
                                        "type": "text",
                                        "text": ", "
                                    },
                                    {
                                        "type": "link",
                                        "url": "https:\/\/arxiv.org\/abs\/2010.07003",
                                        "text": "Length-Adaptive Transformer"
                                    },
                                    {
                                        "type": "text",
                                        "text": ", "
                                    },
                                    {
                                        "type": "link",
                                        "url": "https:\/\/arxiv.org\/abs\/2012.09852",
                                        "text": "SpAtten"
                                    },
                                    {
                                        "type": "text",
                                        "text": ", "
                                    },
                                    {
                                        "type": "link",
                                        "url": "https:\/\/arxiv.org\/abs\/2107.00910",
                                        "text": "Learned Token Pruning"
                                    },
                                    {
                                        "type": "text",
                                        "text": ", "
                                    },
                                    {
                                        "type": "link",
                                        "url": "https:\/\/arxiv.org\/abs\/2210.05832",
                                        "text": "SaiT"
                                    }
                                ]
                            }
                        ],
                        "style": "bullet",
                        "indent": 1,
                        "border": 0
                    },
                    {
                        "type": "rich_text_list",
                        "elements": [
                            {
                                "type": "rich_text_section",
                                "elements": [
                                    {
                                        "type": "text",
                                        "text": "핵심 아이디어: 모든 층의 attention weight를 예측하는 별도의 작은 모델을 훈련하고, 그렇게 찾은 attention weight들을 사용해 최종 token [CLS]에 영향을 크게 미치는 정도를 위쪽 층부터 아래쪽 층까지 back-tracking합니다."
                                    }
                                ]
                            }
                        ],
                        "style": "bullet",
                        "indent": 0,
                        "border": 0
                    },
                    {
                        "type": "rich_text_list",
                        "elements": [
                            {
                                "type": "rich_text_section",
                                "elements": [
                                    {
                                        "type": "text",
                                        "text": "이 논문은 이미 pruning 없이 훈련된 모델에 대해, 모델을 추가로 훈련하지는 않고 inference 속도만 높이는 것이 목적입니다."
                                    }
                                ]
                            },
                            {
                                "type": "rich_text_section",
                                "elements": [
                                    {
                                        "type": "text",
                                        "text": "위에서 언급한 '작은 모델'은 이미 있는 본 모델의 attention weight를 따라가도록 knowledge distillation으로 훈련됩니다."
                                    }
                                ]
                            },
                            {
                                "type": "rich_text_section",
                                "elements": [
                                    {
                                        "type": "text",
                                        "text": "CV(image classification)에서 20%  이하, NLP(GLUE)에서 10% 이하의 token들만 사용해도 성능이 크게 안 떨어진다고 합니다."
                                    }
                                ]
                            }
                        ],
                        "style": "bullet",
                        "indent": 1,
                        "border": 0
                    },
                    {
                        "type": "rich_text_list",
                        "elements": [
                            {
                                "type": "rich_text_section",
                                "elements": [
                                    {
                                        "type": "text",
                                        "text": "의견: 실제로 얼마나 빨라지는지는 별개로, 기존 방법의 문제점을 잘 다룬 것 같습니다."
                                    }
                                ]
                            }
                        ],
                        "style": "bullet",
                        "indent": 0,
                        "border": 0
                    },
                    {
                        "type": "rich_text_list",
                        "elements": [
                            {
                                "type": "rich_text_section",
                                "elements": [
                                    {
                                        "type": "text",
                                        "text": "모든 token에서 output을 내 놔야 하는 문제에 대해서는 token pruning이 깔끔한 방책을 못 내고 있는 것 같습니다. 위의 Length-Adaptive Transformer가 그나마 해결책을 내긴 합니다."
                                    }
                                ]
                            },
                            {
                                "type": "rich_text_section",
                                "elements": [
                                    {
                                        "type": "text",
                                        "text": "GPT와 같은 auto-regressive Transformer에서도 이런 token purning이 사용될 수 있을지 보는 것도 흥미로운 주제라고 생각합니다."
                                    }
                                ]
                            },
                            {
                                "type": "rich_text_section",
                                "elements": [
                                    {
                                        "type": "text",
                                        "text": "최종 output에 각 token이 얼마나 기여하는지를 알아내고자 한다는 점에서, 목적은 다르지만 attention map visualization과도 접점이 있을 것 같습니다."
                                    }
                                ]
                            }
                        ],
                        "style": "bullet",
                        "indent": 1,
                        "border": 0
                    }
                ]
            }
        ],
        "team": "T04MCQMEXQ9",
        "user_team": "T04MCQMEXQ9",
        "source_team": "T04MCQMEXQ9",
        "user_profile": {
            "avatar_hash": "3e494054f705",
            "image_72": "https:\/\/avatars.slack-edge.com\/2023-01-28\/4714393981186_3e494054f7050068883f_72.png",
            "first_name": "심규홍\/퀄컴",
            "real_name": "심규홍\/퀄컴",
            "display_name": "",
            "team": "T04MCQMEXQ9",
            "name": "khshim20",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "edited": {
            "user": "U04MQ2J38JC",
            "ts": "1676213255.000000"
        },
        "reactions": [
            {
                "name": "+1",
                "users": [
                    "U04M09VRR3L",
                    "U04M2NY6U2Y",
                    "U04M0CV8DHR",
                    "U04M091M3MZ",
                    "U04M05UHX7U",
                    "U04LXBEEHMK",
                    "U04LKU43D7Z",
                    "U04MQHDJ12L",
                    "U04NQP19QJ0",
                    "U04M31F24CU",
                    "U04LTMVAQ4W",
                    "U04M0CJUUCT",
                    "U04LYGJ3945",
                    "U04MQ1B2BGQ",
                    "U04M3UUAZTN",
                    "U04LTNK3K46",
                    "U04N5PG7NPJ",
                    "U04MQ90NYFJ",
                    "U04MR94VA68",
                    "U04LXKFJLFP",
                    "U04MD0HNM4Z",
                    "U04LL5UHC5D",
                    "U04LTPY7LES",
                    "U04LY25D7C5",
                    "U04M0B0PWMQ"
                ],
                "count": 25
            }
        ]
    },
    {
        "type": "message",
        "user": "U04MQ2J38JC",
        "upload": false,
        "ts": "1676213255.000000",
        "text": "Transformer 모델의 속도를 높이기 위해, 덜 중요한 token들을 제거해 나가면서 추론하는 방법(a.k.a. token pruning)이 활발히 연구되고 있습니다. 이 중 이번 ICLR에 발표될 논문을 하나 가져왔습니다:\n<https:\/\/openreview.net\/forum?id=VV0hSE8AxCw|Sparse Token Transformers with Attention Back Tracking (ICLR 2023)>\n• 기존 방법: 이전 층에서 attention weight가 작았던 token들을 제거하고 남은 token들로 다음 층 연산을 수행합니다. \n    ◦ 이런 접근 방식은 최종 output에 미칠 영향을 전체적으로 고려하지 못한다는 단점이 있고, 몇 개나\/어떤 기준으로 제거해야 하는지가 연구의 중요한 부분이었습니다.\n    ◦ <https:\/\/arxiv.org\/abs\/2001.08950|PoWER-BERT>, <https:\/\/arxiv.org\/abs\/2010.07003|Length-Adaptive Transformer>, <https:\/\/arxiv.org\/abs\/2012.09852|SpAtten>, <https:\/\/arxiv.org\/abs\/2107.00910|Learned Token Pruning>, <https:\/\/arxiv.org\/abs\/2210.05832|SaiT>\n• 핵심 아이디어: 모든 층의 attention weight를 예측하는 별도의 작은 모델을 훈련하고, 그렇게 찾은 attention weight들을 사용해 최종 token [CLS]에 영향을 크게 미치는 정도를 위쪽 층부터 아래쪽 층까지 back-tracking합니다.\n    ◦ 이 논문은 이미 pruning 없이 훈련된 모델에 대해, 모델을 추가로 훈련하지는 않고 inference 속도만 높이는 것이 목적입니다.\n    ◦ 위에서 언급한 '작은 모델'은 이미 있는 본 모델의 attention weight를 따라가도록 knowledge distillation으로 훈련됩니다.\n    ◦ CV(image classification)에서 20%  이하, NLP(GLUE)에서 10% 이하의 token들만 사용해도 성능이 크게 안 떨어진다고 합니다.\n• 의견: 실제로 얼마나 빨라지는지는 별개로, 기존 방법의 문제점을 잘 다룬 것 같습니다.\n    ◦ 모든 token에서 output을 내 놔야 하는 문제에 대해서는 token pruning이 깔끔한 방책을 못 내고 있는 것 같습니다. 위의 Length-Adaptive Transformer가 그나마 해결책을 내긴 합니다.\n    ◦ GPT와 같은 auto-regressive Transformer에서도 이런 token purning이 사용될 수 있을지 보는 것도 흥미로운 주제라고 생각합니다.\n    ◦ 최종 output에 각 token이 얼마나 기여하는지를 알아내고자 한다는 점에서, 목적은 다르지만 attention map visualization과도 접점이 있을 것 같습니다.",
        "subtype": "message_changed",
        "editor_id": "U04MQ2J38JC",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "3BpE",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "Transformer 모델의 속도를 높이기 위해, 덜 중요한 token들을 제거해 나가면서 추론하는 방법(a.k.a. token pruning)이 활발히 연구되고 있습니다. 이 중 이번 ICLR에 발표될 논문을 하나 가져왔습니다:\n"
                            },
                            {
                                "type": "link",
                                "url": "https:\/\/openreview.net\/forum?id=VV0hSE8AxCw",
                                "text": "Sparse Token Transformers with Attention Back Tracking (ICLR 2023)"
                            },
                            {
                                "type": "text",
                                "text": "\n"
                            }
                        ]
                    },
                    {
                        "type": "rich_text_list",
                        "elements": [
                            {
                                "type": "rich_text_section",
                                "elements": [
                                    {
                                        "type": "text",
                                        "text": "기존 방법: 이전 층에서 attention weight가 작았던 token들을 제거하고 남은 token들로 다음 층 연산을 수행합니다. "
                                    }
                                ]
                            }
                        ],
                        "style": "bullet",
                        "indent": 0,
                        "border": 0
                    },
                    {
                        "type": "rich_text_list",
                        "elements": [
                            {
                                "type": "rich_text_section",
                                "elements": [
                                    {
                                        "type": "text",
                                        "text": "이런 접근 방식은 최종 output에 미칠 영향을 전체적으로 고려하지 못한다는 단점이 있고, 몇 개나\/어떤 기준으로 제거해야 하는지가 연구의 중요한 부분이었습니다."
                                    }
                                ]
                            },
                            {
                                "type": "rich_text_section",
                                "elements": [
                                    {
                                        "type": "link",
                                        "url": "https:\/\/arxiv.org\/abs\/2001.08950",
                                        "text": "PoWER-BERT"
                                    },
                                    {
                                        "type": "text",
                                        "text": ", "
                                    },
                                    {
                                        "type": "link",
                                        "url": "https:\/\/arxiv.org\/abs\/2010.07003",
                                        "text": "Length-Adaptive Transformer"
                                    },
                                    {
                                        "type": "text",
                                        "text": ", "
                                    },
                                    {
                                        "type": "link",
                                        "url": "https:\/\/arxiv.org\/abs\/2012.09852",
                                        "text": "SpAtten"
                                    },
                                    {
                                        "type": "text",
                                        "text": ", "
                                    },
                                    {
                                        "type": "link",
                                        "url": "https:\/\/arxiv.org\/abs\/2107.00910",
                                        "text": "Learned Token Pruning"
                                    },
                                    {
                                        "type": "text",
                                        "text": ", "
                                    },
                                    {
                                        "type": "link",
                                        "url": "https:\/\/arxiv.org\/abs\/2210.05832",
                                        "text": "SaiT"
                                    }
                                ]
                            }
                        ],
                        "style": "bullet",
                        "indent": 1,
                        "border": 0
                    },
                    {
                        "type": "rich_text_list",
                        "elements": [
                            {
                                "type": "rich_text_section",
                                "elements": [
                                    {
                                        "type": "text",
                                        "text": "핵심 아이디어: 모든 층의 attention weight를 예측하는 별도의 작은 모델을 훈련하고, 그렇게 찾은 attention weight들을 사용해 최종 token [CLS]에 영향을 크게 미치는 정도를 위쪽 층부터 아래쪽 층까지 back-tracking합니다."
                                    }
                                ]
                            }
                        ],
                        "style": "bullet",
                        "indent": 0,
                        "border": 0
                    },
                    {
                        "type": "rich_text_list",
                        "elements": [
                            {
                                "type": "rich_text_section",
                                "elements": [
                                    {
                                        "type": "text",
                                        "text": "이 논문은 이미 pruning 없이 훈련된 모델에 대해, 모델을 추가로 훈련하지는 않고 inference 속도만 높이는 것이 목적입니다."
                                    }
                                ]
                            },
                            {
                                "type": "rich_text_section",
                                "elements": [
                                    {
                                        "type": "text",
                                        "text": "위에서 언급한 '작은 모델'은 이미 있는 본 모델의 attention weight를 따라가도록 knowledge distillation으로 훈련됩니다."
                                    }
                                ]
                            },
                            {
                                "type": "rich_text_section",
                                "elements": [
                                    {
                                        "type": "text",
                                        "text": "CV(image classification)에서 20%  이하, NLP(GLUE)에서 10% 이하의 token들만 사용해도 성능이 크게 안 떨어진다고 합니다."
                                    }
                                ]
                            }
                        ],
                        "style": "bullet",
                        "indent": 1,
                        "border": 0
                    },
                    {
                        "type": "rich_text_list",
                        "elements": [
                            {
                                "type": "rich_text_section",
                                "elements": [
                                    {
                                        "type": "text",
                                        "text": "의견: 실제로 얼마나 빨라지는지는 별개로, 기존 방법의 문제점을 잘 다룬 것 같습니다."
                                    }
                                ]
                            }
                        ],
                        "style": "bullet",
                        "indent": 0,
                        "border": 0
                    },
                    {
                        "type": "rich_text_list",
                        "elements": [
                            {
                                "type": "rich_text_section",
                                "elements": [
                                    {
                                        "type": "text",
                                        "text": "모든 token에서 output을 내 놔야 하는 문제에 대해서는 token pruning이 깔끔한 방책을 못 내고 있는 것 같습니다. 위의 Length-Adaptive Transformer가 그나마 해결책을 내긴 합니다."
                                    }
                                ]
                            },
                            {
                                "type": "rich_text_section",
                                "elements": [
                                    {
                                        "type": "text",
                                        "text": "GPT와 같은 auto-regressive Transformer에서도 이런 token purning이 사용될 수 있을지 보는 것도 흥미로운 주제라고 생각합니다."
                                    }
                                ]
                            },
                            {
                                "type": "rich_text_section",
                                "elements": [
                                    {
                                        "type": "text",
                                        "text": "최종 output에 각 token이 얼마나 기여하는지를 알아내고자 한다는 점에서, 목적은 다르지만 attention map visualization과도 접점이 있을 것 같습니다."
                                    }
                                ]
                            }
                        ],
                        "style": "bullet",
                        "indent": 1,
                        "border": 0
                    }
                ]
            }
        ],
        "original": {
            "client_msg_id": "0a49456a-3d2d-46d7-bccd-312376db5b3d",
            "type": "message",
            "text": "Transformer 모델의 속도를 높이기 위해, 덜 중요한 token들을 제거해 나가면서 추론하는 방법(a.k.a. token pruning)이 활발히 연구되고 있습니다. 이 중 이번 ICLR에 발표될 논문입니다:\n<https:\/\/openreview.net\/forum?id=VV0hSE8AxCw|Sparse Token Transformers with Attention Back Tracking (ICLR 2023)>\n• 기존 방법: 이전 층에서 attention weight가 작았던 token들을 제거하고 남은 token들로 다음 층 연산을 수행합니다. \n    ◦ 이런 접근 방식은 최종 output에 미칠 영향을 전체적으로 고려하지 못한다는 단점이 있고, 몇 개나\/어떤 기준으로 제거해야 하는지가 연구의 중요한 부분이었습니다.\n    ◦ <https:\/\/arxiv.org\/abs\/2001.08950|PoWER-BERT>, <https:\/\/arxiv.org\/abs\/2010.07003|Length-Adaptive Transformer>, <https:\/\/arxiv.org\/abs\/2012.09852|SpAtten>, <https:\/\/arxiv.org\/abs\/2107.00910|Learned Token Pruning>, <https:\/\/arxiv.org\/abs\/2210.05832|SaiT>\n• 핵심 아이디어: 모든 층의 attention weight를 예측하는 별도의 작은 모델을 훈련하고, 그렇게 찾은 attention weight들을 사용해 최종 token [CLS]에 영향을 크게 미치는 정도를 위쪽 층부터 아래쪽 층까지 back-tracking합니다.\n    ◦ 이 논문은 이미 pruning 없이 훈련된 모델에 대해, 모델을 추가로 훈련하지는 않고 inference 속도만 높이는 것이 목적입니다.\n    ◦ 위에서 언급한 '작은 모델'은 이미 있는 본 모델의 attention weight를 따라가도록 knowledge distillation으로 훈련됩니다.\n    ◦ CV(image classification)에서 20%  이하, NLP(GLUE)에서 10% 이하의 token들만 사용해도 성능이 크게 안 떨어진다고 합니다.\n• 의견: 실제로 얼마나 빨라지는지는 별개로, 기존 방법의 문제점을 잘 다룬 것 같습니다.\n    ◦ 모든 token에서 output을 내 놔야 하는 문제에 대해서는 token pruning이 깔끔한 방책을 못 내고 있는 것 같습니다. 위의 Length-Adaptive Transformer가 그나마 해결책을 내긴 합니다.\n    ◦ GPT와 같은 auto-regressive Transformer에서도 이런 token purning이 사용될 수 있을지 보는 것도 흥미로운 주제라고 생각합니다.\n    ◦ 최종 output에 각 token이 얼마나 기여하는지를 알아내고자 한다는 점에서, 목적은 다르지만 attention map visualization과도 접점이 있을 것 같습니다.",
            "user": "U04MQ2J38JC",
            "ts": "1676212879.655989",
            "blocks": [
                {
                    "type": "rich_text",
                    "block_id": "9HTgo",
                    "elements": [
                        {
                            "type": "rich_text_section",
                            "elements": [
                                {
                                    "type": "text",
                                    "text": "Transformer 모델의 속도를 높이기 위해, 덜 중요한 token들을 제거해 나가면서 추론하는 방법(a.k.a. token pruning)이 활발히 연구되고 있습니다. 이 중 이번 ICLR에 발표될 논문입니다:\n"
                                },
                                {
                                    "type": "link",
                                    "url": "https:\/\/openreview.net\/forum?id=VV0hSE8AxCw",
                                    "text": "Sparse Token Transformers with Attention Back Tracking (ICLR 2023)"
                                },
                                {
                                    "type": "text",
                                    "text": "\n"
                                }
                            ]
                        },
                        {
                            "type": "rich_text_list",
                            "elements": [
                                {
                                    "type": "rich_text_section",
                                    "elements": [
                                        {
                                            "type": "text",
                                            "text": "기존 방법: 이전 층에서 attention weight가 작았던 token들을 제거하고 남은 token들로 다음 층 연산을 수행합니다. "
                                        }
                                    ]
                                }
                            ],
                            "style": "bullet",
                            "indent": 0,
                            "border": 0
                        },
                        {
                            "type": "rich_text_list",
                            "elements": [
                                {
                                    "type": "rich_text_section",
                                    "elements": [
                                        {
                                            "type": "text",
                                            "text": "이런 접근 방식은 최종 output에 미칠 영향을 전체적으로 고려하지 못한다는 단점이 있고, 몇 개나\/어떤 기준으로 제거해야 하는지가 연구의 중요한 부분이었습니다."
                                        }
                                    ]
                                },
                                {
                                    "type": "rich_text_section",
                                    "elements": [
                                        {
                                            "type": "link",
                                            "url": "https:\/\/arxiv.org\/abs\/2001.08950",
                                            "text": "PoWER-BERT"
                                        },
                                        {
                                            "type": "text",
                                            "text": ", "
                                        },
                                        {
                                            "type": "link",
                                            "url": "https:\/\/arxiv.org\/abs\/2010.07003",
                                            "text": "Length-Adaptive Transformer"
                                        },
                                        {
                                            "type": "text",
                                            "text": ", "
                                        },
                                        {
                                            "type": "link",
                                            "url": "https:\/\/arxiv.org\/abs\/2012.09852",
                                            "text": "SpAtten"
                                        },
                                        {
                                            "type": "text",
                                            "text": ", "
                                        },
                                        {
                                            "type": "link",
                                            "url": "https:\/\/arxiv.org\/abs\/2107.00910",
                                            "text": "Learned Token Pruning"
                                        },
                                        {
                                            "type": "text",
                                            "text": ", "
                                        },
                                        {
                                            "type": "link",
                                            "url": "https:\/\/arxiv.org\/abs\/2210.05832",
                                            "text": "SaiT"
                                        }
                                    ]
                                }
                            ],
                            "style": "bullet",
                            "indent": 1,
                            "border": 0
                        },
                        {
                            "type": "rich_text_list",
                            "elements": [
                                {
                                    "type": "rich_text_section",
                                    "elements": [
                                        {
                                            "type": "text",
                                            "text": "핵심 아이디어: 모든 층의 attention weight를 예측하는 별도의 작은 모델을 훈련하고, 그렇게 찾은 attention weight들을 사용해 최종 token [CLS]에 영향을 크게 미치는 정도를 위쪽 층부터 아래쪽 층까지 back-tracking합니다."
                                        }
                                    ]
                                }
                            ],
                            "style": "bullet",
                            "indent": 0,
                            "border": 0
                        },
                        {
                            "type": "rich_text_list",
                            "elements": [
                                {
                                    "type": "rich_text_section",
                                    "elements": [
                                        {
                                            "type": "text",
                                            "text": "이 논문은 이미 pruning 없이 훈련된 모델에 대해, 모델을 추가로 훈련하지는 않고 inference 속도만 높이는 것이 목적입니다."
                                        }
                                    ]
                                },
                                {
                                    "type": "rich_text_section",
                                    "elements": [
                                        {
                                            "type": "text",
                                            "text": "위에서 언급한 '작은 모델'은 이미 있는 본 모델의 attention weight를 따라가도록 knowledge distillation으로 훈련됩니다."
                                        }
                                    ]
                                },
                                {
                                    "type": "rich_text_section",
                                    "elements": [
                                        {
                                            "type": "text",
                                            "text": "CV(image classification)에서 20%  이하, NLP(GLUE)에서 10% 이하의 token들만 사용해도 성능이 크게 안 떨어진다고 합니다."
                                        }
                                    ]
                                }
                            ],
                            "style": "bullet",
                            "indent": 1,
                            "border": 0
                        },
                        {
                            "type": "rich_text_list",
                            "elements": [
                                {
                                    "type": "rich_text_section",
                                    "elements": [
                                        {
                                            "type": "text",
                                            "text": "의견: 실제로 얼마나 빨라지는지는 별개로, 기존 방법의 문제점을 잘 다룬 것 같습니다."
                                        }
                                    ]
                                }
                            ],
                            "style": "bullet",
                            "indent": 0,
                            "border": 0
                        },
                        {
                            "type": "rich_text_list",
                            "elements": [
                                {
                                    "type": "rich_text_section",
                                    "elements": [
                                        {
                                            "type": "text",
                                            "text": "모든 token에서 output을 내 놔야 하는 문제에 대해서는 token pruning이 깔끔한 방책을 못 내고 있는 것 같습니다. 위의 Length-Adaptive Transformer가 그나마 해결책을 내긴 합니다."
                                        }
                                    ]
                                },
                                {
                                    "type": "rich_text_section",
                                    "elements": [
                                        {
                                            "type": "text",
                                            "text": "GPT와 같은 auto-regressive Transformer에서도 이런 token purning이 사용될 수 있을지 보는 것도 흥미로운 주제라고 생각합니다."
                                        }
                                    ]
                                },
                                {
                                    "type": "rich_text_section",
                                    "elements": [
                                        {
                                            "type": "text",
                                            "text": "최종 output에 각 token이 얼마나 기여하는지를 알아내고자 한다는 점에서, 목적은 다르지만 attention map visualization과도 접점이 있을 것 같습니다."
                                        }
                                    ]
                                }
                            ],
                            "style": "bullet",
                            "indent": 1,
                            "border": 0
                        }
                    ]
                }
            ],
            "team": "T04MCQMEXQ9",
            "user_team": "T04MCQMEXQ9",
            "source_team": "T04MCQMEXQ9",
            "user_profile": {
                "avatar_hash": "3e494054f705",
                "image_72": "https:\/\/avatars.slack-edge.com\/2023-01-28\/4714393981186_3e494054f7050068883f_72.png",
                "first_name": "심규홍\/퀄컴",
                "real_name": "심규홍\/퀄컴",
                "display_name": "",
                "team": "T04MCQMEXQ9",
                "name": "khshim20",
                "is_restricted": false,
                "is_ultra_restricted": false
            },
            "edited": {
                "user": "U04MQ2J38JC",
                "ts": "1676213050.000000"
            }
        },
        "edited_by": "U04MQ2J38JC"
    },
    {
        "type": "message",
        "user": "U04MQ2J38JC",
        "upload": false,
        "ts": "1676213050.000000",
        "text": "Transformer 모델의 속도를 높이기 위해, 덜 중요한 token들을 제거해 나가면서 추론하는 방법(a.k.a. token pruning)이 활발히 연구되고 있습니다. 이 중 이번 ICLR에 발표될 논문입니다:\n<https:\/\/openreview.net\/forum?id=VV0hSE8AxCw|Sparse Token Transformers with Attention Back Tracking (ICLR 2023)>\n• 기존 방법: 이전 층에서 attention weight가 작았던 token들을 제거하고 남은 token들로 다음 층 연산을 수행합니다. \n    ◦ 이런 접근 방식은 최종 output에 미칠 영향을 전체적으로 고려하지 못한다는 단점이 있고, 몇 개나\/어떤 기준으로 제거해야 하는지가 연구의 중요한 부분이었습니다.\n    ◦ <https:\/\/arxiv.org\/abs\/2001.08950|PoWER-BERT>, <https:\/\/arxiv.org\/abs\/2010.07003|Length-Adaptive Transformer>, <https:\/\/arxiv.org\/abs\/2012.09852|SpAtten>, <https:\/\/arxiv.org\/abs\/2107.00910|Learned Token Pruning>, <https:\/\/arxiv.org\/abs\/2210.05832|SaiT>\n• 핵심 아이디어: 모든 층의 attention weight를 예측하는 별도의 작은 모델을 훈련하고, 그렇게 찾은 attention weight들을 사용해 최종 token [CLS]에 영향을 크게 미치는 정도를 위쪽 층부터 아래쪽 층까지 back-tracking합니다.\n    ◦ 이 논문은 이미 pruning 없이 훈련된 모델에 대해, 모델을 추가로 훈련하지는 않고 inference 속도만 높이는 것이 목적입니다.\n    ◦ 위에서 언급한 '작은 모델'은 이미 있는 본 모델의 attention weight를 따라가도록 knowledge distillation으로 훈련됩니다.\n    ◦ CV(image classification)에서 20%  이하, NLP(GLUE)에서 10% 이하의 token들만 사용해도 성능이 크게 안 떨어진다고 합니다.\n• 의견: 실제로 얼마나 빨라지는지는 별개로, 기존 방법의 문제점을 잘 다룬 것 같습니다.\n    ◦ 모든 token에서 output을 내 놔야 하는 문제에 대해서는 token pruning이 깔끔한 방책을 못 내고 있는 것 같습니다. 위의 Length-Adaptive Transformer가 그나마 해결책을 내긴 합니다.\n    ◦ GPT와 같은 auto-regressive Transformer에서도 이런 token purning이 사용될 수 있을지 보는 것도 흥미로운 주제라고 생각합니다.\n    ◦ 최종 output에 각 token이 얼마나 기여하는지를 알아내고자 한다는 점에서, 목적은 다르지만 attention map visualization과도 접점이 있을 것 같습니다.",
        "subtype": "message_changed",
        "editor_id": "U04MQ2J38JC",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "9HTgo",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "Transformer 모델의 속도를 높이기 위해, 덜 중요한 token들을 제거해 나가면서 추론하는 방법(a.k.a. token pruning)이 활발히 연구되고 있습니다. 이 중 이번 ICLR에 발표될 논문입니다:\n"
                            },
                            {
                                "type": "link",
                                "url": "https:\/\/openreview.net\/forum?id=VV0hSE8AxCw",
                                "text": "Sparse Token Transformers with Attention Back Tracking (ICLR 2023)"
                            },
                            {
                                "type": "text",
                                "text": "\n"
                            }
                        ]
                    },
                    {
                        "type": "rich_text_list",
                        "elements": [
                            {
                                "type": "rich_text_section",
                                "elements": [
                                    {
                                        "type": "text",
                                        "text": "기존 방법: 이전 층에서 attention weight가 작았던 token들을 제거하고 남은 token들로 다음 층 연산을 수행합니다. "
                                    }
                                ]
                            }
                        ],
                        "style": "bullet",
                        "indent": 0,
                        "border": 0
                    },
                    {
                        "type": "rich_text_list",
                        "elements": [
                            {
                                "type": "rich_text_section",
                                "elements": [
                                    {
                                        "type": "text",
                                        "text": "이런 접근 방식은 최종 output에 미칠 영향을 전체적으로 고려하지 못한다는 단점이 있고, 몇 개나\/어떤 기준으로 제거해야 하는지가 연구의 중요한 부분이었습니다."
                                    }
                                ]
                            },
                            {
                                "type": "rich_text_section",
                                "elements": [
                                    {
                                        "type": "link",
                                        "url": "https:\/\/arxiv.org\/abs\/2001.08950",
                                        "text": "PoWER-BERT"
                                    },
                                    {
                                        "type": "text",
                                        "text": ", "
                                    },
                                    {
                                        "type": "link",
                                        "url": "https:\/\/arxiv.org\/abs\/2010.07003",
                                        "text": "Length-Adaptive Transformer"
                                    },
                                    {
                                        "type": "text",
                                        "text": ", "
                                    },
                                    {
                                        "type": "link",
                                        "url": "https:\/\/arxiv.org\/abs\/2012.09852",
                                        "text": "SpAtten"
                                    },
                                    {
                                        "type": "text",
                                        "text": ", "
                                    },
                                    {
                                        "type": "link",
                                        "url": "https:\/\/arxiv.org\/abs\/2107.00910",
                                        "text": "Learned Token Pruning"
                                    },
                                    {
                                        "type": "text",
                                        "text": ", "
                                    },
                                    {
                                        "type": "link",
                                        "url": "https:\/\/arxiv.org\/abs\/2210.05832",
                                        "text": "SaiT"
                                    }
                                ]
                            }
                        ],
                        "style": "bullet",
                        "indent": 1,
                        "border": 0
                    },
                    {
                        "type": "rich_text_list",
                        "elements": [
                            {
                                "type": "rich_text_section",
                                "elements": [
                                    {
                                        "type": "text",
                                        "text": "핵심 아이디어: 모든 층의 attention weight를 예측하는 별도의 작은 모델을 훈련하고, 그렇게 찾은 attention weight들을 사용해 최종 token [CLS]에 영향을 크게 미치는 정도를 위쪽 층부터 아래쪽 층까지 back-tracking합니다."
                                    }
                                ]
                            }
                        ],
                        "style": "bullet",
                        "indent": 0,
                        "border": 0
                    },
                    {
                        "type": "rich_text_list",
                        "elements": [
                            {
                                "type": "rich_text_section",
                                "elements": [
                                    {
                                        "type": "text",
                                        "text": "이 논문은 이미 pruning 없이 훈련된 모델에 대해, 모델을 추가로 훈련하지는 않고 inference 속도만 높이는 것이 목적입니다."
                                    }
                                ]
                            },
                            {
                                "type": "rich_text_section",
                                "elements": [
                                    {
                                        "type": "text",
                                        "text": "위에서 언급한 '작은 모델'은 이미 있는 본 모델의 attention weight를 따라가도록 knowledge distillation으로 훈련됩니다."
                                    }
                                ]
                            },
                            {
                                "type": "rich_text_section",
                                "elements": [
                                    {
                                        "type": "text",
                                        "text": "CV(image classification)에서 20%  이하, NLP(GLUE)에서 10% 이하의 token들만 사용해도 성능이 크게 안 떨어진다고 합니다."
                                    }
                                ]
                            }
                        ],
                        "style": "bullet",
                        "indent": 1,
                        "border": 0
                    },
                    {
                        "type": "rich_text_list",
                        "elements": [
                            {
                                "type": "rich_text_section",
                                "elements": [
                                    {
                                        "type": "text",
                                        "text": "의견: 실제로 얼마나 빨라지는지는 별개로, 기존 방법의 문제점을 잘 다룬 것 같습니다."
                                    }
                                ]
                            }
                        ],
                        "style": "bullet",
                        "indent": 0,
                        "border": 0
                    },
                    {
                        "type": "rich_text_list",
                        "elements": [
                            {
                                "type": "rich_text_section",
                                "elements": [
                                    {
                                        "type": "text",
                                        "text": "모든 token에서 output을 내 놔야 하는 문제에 대해서는 token pruning이 깔끔한 방책을 못 내고 있는 것 같습니다. 위의 Length-Adaptive Transformer가 그나마 해결책을 내긴 합니다."
                                    }
                                ]
                            },
                            {
                                "type": "rich_text_section",
                                "elements": [
                                    {
                                        "type": "text",
                                        "text": "GPT와 같은 auto-regressive Transformer에서도 이런 token purning이 사용될 수 있을지 보는 것도 흥미로운 주제라고 생각합니다."
                                    }
                                ]
                            },
                            {
                                "type": "rich_text_section",
                                "elements": [
                                    {
                                        "type": "text",
                                        "text": "최종 output에 각 token이 얼마나 기여하는지를 알아내고자 한다는 점에서, 목적은 다르지만 attention map visualization과도 접점이 있을 것 같습니다."
                                    }
                                ]
                            }
                        ],
                        "style": "bullet",
                        "indent": 1,
                        "border": 0
                    }
                ]
            }
        ],
        "original": {
            "client_msg_id": "0a49456a-3d2d-46d7-bccd-312376db5b3d",
            "type": "message",
            "text": "Transformer 모델의 속도를 높이기 위해, 덜 중요한 token들을 제거해 나가면서 추론하는 방법(a.k.a. token pruning)이 활발히 연구되고 있습니다. 이 중 이번 ICLR에 발표될 논문입니다:\n<https:\/\/openreview.net\/forum?id=VV0hSE8AxCw|Sparse Token Transformers with Attention Back Tracking (ICLR 2023)>\n• 기존 방법: 이전 층에서 attention weight가 작았던 token들을 제거하고 남은 token들로 다음 층 연산을 수행합니다. \n    ◦ 이런 접근 방식은 최종 output에 미칠 영향을 전체적으로 고려하지 못한다는 단점이 있고, 몇 개나\/어떤 기준으로 제거해야 하는지가 연구의 중요한 부분이었습니다.\n    ◦ <https:\/\/arxiv.org\/abs\/2001.08950|PoWER-BERT>, <https:\/\/arxiv.org\/abs\/2010.07003|Length-Adaptive Transformer>, <https:\/\/arxiv.org\/abs\/2012.09852|SpAtten>, <https:\/\/arxiv.org\/abs\/2107.00910|Learned Token Pruning>, <https:\/\/arxiv.org\/abs\/2210.05832|SaiT>\n• 핵심 아이디어: 모든 층의 attention weight를 예측하는 별도의 작은 모델을 훈련하고, 그렇게 찾은 attention weight들을 사용해 최종 token [CLS]에 영향을 크게 미치는 정도를 위쪽 층부터 아래쪽 층까지 back-tracking합니다.\n    ◦ 이 논문은 이미 pruning 없이 훈련된 모델에 대해, 모델을 추가로 훈련하지는 않고 inference 속도만 높이는 것이 목적입니다.\n    ◦ 위에서 언급한 '작은 모델'은 이미 있는 본 모델의 attention weight를 따라가도록 knowledge distillation으로 훈련됩니다.\n    ◦ CV(image classification)에서 20%  이하, NLP(GLUE)에서 10% 이하의 token들만 사용해도 성능이 크게 안 떨어진다고 합니다.\n• 의견: 실제로 얼마나 빨라지는지는 별개로, 기존 방법의 문제점을 잘 다룬 것 같습니다.\n    ◦ 모든 token에서 output을 내 놔야 하는 문제에 대해서는 token pruning이 깔끔한 방책을 못 내고 있는 것 같습니다. 위의 Length-Adaptive Transformer가 그나마 해결책을 내긴 합니다.\n    ◦ GPT와 같은 auto-regressive Transformer에서도 이런 token purning이 사용될 수 있을지 보는 것도 흥미로운 주제입니다.\n    ◦ 최종 output에 각 token이 얼마나 기여하는지를 알아내고자 한다는 점에서, 목적은 다르지만 attention map visualization과도 접점이 있을 것 같습니다.",
            "user": "U04MQ2J38JC",
            "ts": "1676212879.655989",
            "blocks": [
                {
                    "type": "rich_text",
                    "block_id": "MlUS",
                    "elements": [
                        {
                            "type": "rich_text_section",
                            "elements": [
                                {
                                    "type": "text",
                                    "text": "Transformer 모델의 속도를 높이기 위해, 덜 중요한 token들을 제거해 나가면서 추론하는 방법(a.k.a. token pruning)이 활발히 연구되고 있습니다. 이 중 이번 ICLR에 발표될 논문입니다:\n"
                                },
                                {
                                    "type": "link",
                                    "url": "https:\/\/openreview.net\/forum?id=VV0hSE8AxCw",
                                    "text": "Sparse Token Transformers with Attention Back Tracking (ICLR 2023)"
                                },
                                {
                                    "type": "text",
                                    "text": "\n"
                                }
                            ]
                        },
                        {
                            "type": "rich_text_list",
                            "elements": [
                                {
                                    "type": "rich_text_section",
                                    "elements": [
                                        {
                                            "type": "text",
                                            "text": "기존 방법: 이전 층에서 attention weight가 작았던 token들을 제거하고 남은 token들로 다음 층 연산을 수행합니다. "
                                        }
                                    ]
                                }
                            ],
                            "style": "bullet",
                            "indent": 0,
                            "border": 0
                        },
                        {
                            "type": "rich_text_list",
                            "elements": [
                                {
                                    "type": "rich_text_section",
                                    "elements": [
                                        {
                                            "type": "text",
                                            "text": "이런 접근 방식은 최종 output에 미칠 영향을 전체적으로 고려하지 못한다는 단점이 있고, 몇 개나\/어떤 기준으로 제거해야 하는지가 연구의 중요한 부분이었습니다."
                                        }
                                    ]
                                },
                                {
                                    "type": "rich_text_section",
                                    "elements": [
                                        {
                                            "type": "link",
                                            "url": "https:\/\/arxiv.org\/abs\/2001.08950",
                                            "text": "PoWER-BERT"
                                        },
                                        {
                                            "type": "text",
                                            "text": ", "
                                        },
                                        {
                                            "type": "link",
                                            "url": "https:\/\/arxiv.org\/abs\/2010.07003",
                                            "text": "Length-Adaptive Transformer"
                                        },
                                        {
                                            "type": "text",
                                            "text": ", "
                                        },
                                        {
                                            "type": "link",
                                            "url": "https:\/\/arxiv.org\/abs\/2012.09852",
                                            "text": "SpAtten"
                                        },
                                        {
                                            "type": "text",
                                            "text": ", "
                                        },
                                        {
                                            "type": "link",
                                            "url": "https:\/\/arxiv.org\/abs\/2107.00910",
                                            "text": "Learned Token Pruning"
                                        },
                                        {
                                            "type": "text",
                                            "text": ", "
                                        },
                                        {
                                            "type": "link",
                                            "url": "https:\/\/arxiv.org\/abs\/2210.05832",
                                            "text": "SaiT"
                                        }
                                    ]
                                }
                            ],
                            "style": "bullet",
                            "indent": 1,
                            "border": 0
                        },
                        {
                            "type": "rich_text_list",
                            "elements": [
                                {
                                    "type": "rich_text_section",
                                    "elements": [
                                        {
                                            "type": "text",
                                            "text": "핵심 아이디어: 모든 층의 attention weight를 예측하는 별도의 작은 모델을 훈련하고, 그렇게 찾은 attention weight들을 사용해 최종 token [CLS]에 영향을 크게 미치는 정도를 위쪽 층부터 아래쪽 층까지 back-tracking합니다."
                                        }
                                    ]
                                }
                            ],
                            "style": "bullet",
                            "indent": 0,
                            "border": 0
                        },
                        {
                            "type": "rich_text_list",
                            "elements": [
                                {
                                    "type": "rich_text_section",
                                    "elements": [
                                        {
                                            "type": "text",
                                            "text": "이 논문은 이미 pruning 없이 훈련된 모델에 대해, 모델을 추가로 훈련하지는 않고 inference 속도만 높이는 것이 목적입니다."
                                        }
                                    ]
                                },
                                {
                                    "type": "rich_text_section",
                                    "elements": [
                                        {
                                            "type": "text",
                                            "text": "위에서 언급한 '작은 모델'은 이미 있는 본 모델의 attention weight를 따라가도록 knowledge distillation으로 훈련됩니다."
                                        }
                                    ]
                                },
                                {
                                    "type": "rich_text_section",
                                    "elements": [
                                        {
                                            "type": "text",
                                            "text": "CV(image classification)에서 20%  이하, NLP(GLUE)에서 10% 이하의 token들만 사용해도 성능이 크게 안 떨어진다고 합니다."
                                        }
                                    ]
                                }
                            ],
                            "style": "bullet",
                            "indent": 1,
                            "border": 0
                        },
                        {
                            "type": "rich_text_list",
                            "elements": [
                                {
                                    "type": "rich_text_section",
                                    "elements": [
                                        {
                                            "type": "text",
                                            "text": "의견: 실제로 얼마나 빨라지는지는 별개로, 기존 방법의 문제점을 잘 다룬 것 같습니다."
                                        }
                                    ]
                                }
                            ],
                            "style": "bullet",
                            "indent": 0,
                            "border": 0
                        },
                        {
                            "type": "rich_text_list",
                            "elements": [
                                {
                                    "type": "rich_text_section",
                                    "elements": [
                                        {
                                            "type": "text",
                                            "text": "모든 token에서 output을 내 놔야 하는 문제에 대해서는 token pruning이 깔끔한 방책을 못 내고 있는 것 같습니다. 위의 Length-Adaptive Transformer가 그나마 해결책을 내긴 합니다."
                                        }
                                    ]
                                },
                                {
                                    "type": "rich_text_section",
                                    "elements": [
                                        {
                                            "type": "text",
                                            "text": "GPT와 같은 auto-regressive Transformer에서도 이런 token purning이 사용될 수 있을지 보는 것도 흥미로운 주제입니다."
                                        }
                                    ]
                                },
                                {
                                    "type": "rich_text_section",
                                    "elements": [
                                        {
                                            "type": "text",
                                            "text": "최종 output에 각 token이 얼마나 기여하는지를 알아내고자 한다는 점에서, 목적은 다르지만 attention map visualization과도 접점이 있을 것 같습니다."
                                        }
                                    ]
                                }
                            ],
                            "style": "bullet",
                            "indent": 1,
                            "border": 0
                        }
                    ]
                }
            ],
            "team": "T04MCQMEXQ9",
            "user_team": "T04MCQMEXQ9",
            "source_team": "T04MCQMEXQ9",
            "user_profile": {
                "avatar_hash": "3e494054f705",
                "image_72": "https:\/\/avatars.slack-edge.com\/2023-01-28\/4714393981186_3e494054f7050068883f_72.png",
                "first_name": "심규홍\/퀄컴",
                "real_name": "심규홍\/퀄컴",
                "display_name": "",
                "team": "T04MCQMEXQ9",
                "name": "khshim20",
                "is_restricted": false,
                "is_ultra_restricted": false
            },
            "edited": {
                "user": "U04MQ2J38JC",
                "ts": "1676213031.000000"
            }
        },
        "edited_by": "U04MQ2J38JC"
    },
    {
        "type": "message",
        "user": "U04MQ2J38JC",
        "upload": false,
        "ts": "1676213031.000000",
        "text": "Transformer 모델의 속도를 높이기 위해, 덜 중요한 token들을 제거해 나가면서 추론하는 방법(a.k.a. token pruning)이 활발히 연구되고 있습니다. 이 중 이번 ICLR에 발표될 논문입니다:\n<https:\/\/openreview.net\/forum?id=VV0hSE8AxCw|Sparse Token Transformers with Attention Back Tracking (ICLR 2023)>\n• 기존 방법: 이전 층에서 attention weight가 작았던 token들을 제거하고 남은 token들로 다음 층 연산을 수행합니다. \n    ◦ 이런 접근 방식은 최종 output에 미칠 영향을 전체적으로 고려하지 못한다는 단점이 있고, 몇 개나\/어떤 기준으로 제거해야 하는지가 연구의 중요한 부분이었습니다.\n    ◦ <https:\/\/arxiv.org\/abs\/2001.08950|PoWER-BERT>, <https:\/\/arxiv.org\/abs\/2010.07003|Length-Adaptive Transformer>, <https:\/\/arxiv.org\/abs\/2012.09852|SpAtten>, <https:\/\/arxiv.org\/abs\/2107.00910|Learned Token Pruning>, <https:\/\/arxiv.org\/abs\/2210.05832|SaiT>\n• 핵심 아이디어: 모든 층의 attention weight를 예측하는 별도의 작은 모델을 훈련하고, 그렇게 찾은 attention weight들을 사용해 최종 token [CLS]에 영향을 크게 미치는 정도를 위쪽 층부터 아래쪽 층까지 back-tracking합니다.\n    ◦ 이 논문은 이미 pruning 없이 훈련된 모델에 대해, 모델을 추가로 훈련하지는 않고 inference 속도만 높이는 것이 목적입니다.\n    ◦ 위에서 언급한 '작은 모델'은 이미 있는 본 모델의 attention weight를 따라가도록 knowledge distillation으로 훈련됩니다.\n    ◦ CV(image classification)에서 20%  이하, NLP(GLUE)에서 10% 이하의 token들만 사용해도 성능이 크게 안 떨어진다고 합니다.\n• 의견: 실제로 얼마나 빨라지는지는 별개로, 기존 방법의 문제점을 잘 다룬 것 같습니다.\n    ◦ 모든 token에서 output을 내 놔야 하는 문제에 대해서는 token pruning이 깔끔한 방책을 못 내고 있는 것 같습니다. 위의 Length-Adaptive Transformer가 그나마 해결책을 내긴 합니다.\n    ◦ GPT와 같은 auto-regressive Transformer에서도 이런 token purning이 사용될 수 있을지 보는 것도 흥미로운 주제입니다.\n    ◦ 최종 output에 각 token이 얼마나 기여하는지를 알아내고자 한다는 점에서, 목적은 다르지만 attention map visualization과도 접점이 있을 것 같습니다.",
        "subtype": "message_changed",
        "editor_id": "U04MQ2J38JC",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "MlUS",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "Transformer 모델의 속도를 높이기 위해, 덜 중요한 token들을 제거해 나가면서 추론하는 방법(a.k.a. token pruning)이 활발히 연구되고 있습니다. 이 중 이번 ICLR에 발표될 논문입니다:\n"
                            },
                            {
                                "type": "link",
                                "url": "https:\/\/openreview.net\/forum?id=VV0hSE8AxCw",
                                "text": "Sparse Token Transformers with Attention Back Tracking (ICLR 2023)"
                            },
                            {
                                "type": "text",
                                "text": "\n"
                            }
                        ]
                    },
                    {
                        "type": "rich_text_list",
                        "elements": [
                            {
                                "type": "rich_text_section",
                                "elements": [
                                    {
                                        "type": "text",
                                        "text": "기존 방법: 이전 층에서 attention weight가 작았던 token들을 제거하고 남은 token들로 다음 층 연산을 수행합니다. "
                                    }
                                ]
                            }
                        ],
                        "style": "bullet",
                        "indent": 0,
                        "border": 0
                    },
                    {
                        "type": "rich_text_list",
                        "elements": [
                            {
                                "type": "rich_text_section",
                                "elements": [
                                    {
                                        "type": "text",
                                        "text": "이런 접근 방식은 최종 output에 미칠 영향을 전체적으로 고려하지 못한다는 단점이 있고, 몇 개나\/어떤 기준으로 제거해야 하는지가 연구의 중요한 부분이었습니다."
                                    }
                                ]
                            },
                            {
                                "type": "rich_text_section",
                                "elements": [
                                    {
                                        "type": "link",
                                        "url": "https:\/\/arxiv.org\/abs\/2001.08950",
                                        "text": "PoWER-BERT"
                                    },
                                    {
                                        "type": "text",
                                        "text": ", "
                                    },
                                    {
                                        "type": "link",
                                        "url": "https:\/\/arxiv.org\/abs\/2010.07003",
                                        "text": "Length-Adaptive Transformer"
                                    },
                                    {
                                        "type": "text",
                                        "text": ", "
                                    },
                                    {
                                        "type": "link",
                                        "url": "https:\/\/arxiv.org\/abs\/2012.09852",
                                        "text": "SpAtten"
                                    },
                                    {
                                        "type": "text",
                                        "text": ", "
                                    },
                                    {
                                        "type": "link",
                                        "url": "https:\/\/arxiv.org\/abs\/2107.00910",
                                        "text": "Learned Token Pruning"
                                    },
                                    {
                                        "type": "text",
                                        "text": ", "
                                    },
                                    {
                                        "type": "link",
                                        "url": "https:\/\/arxiv.org\/abs\/2210.05832",
                                        "text": "SaiT"
                                    }
                                ]
                            }
                        ],
                        "style": "bullet",
                        "indent": 1,
                        "border": 0
                    },
                    {
                        "type": "rich_text_list",
                        "elements": [
                            {
                                "type": "rich_text_section",
                                "elements": [
                                    {
                                        "type": "text",
                                        "text": "핵심 아이디어: 모든 층의 attention weight를 예측하는 별도의 작은 모델을 훈련하고, 그렇게 찾은 attention weight들을 사용해 최종 token [CLS]에 영향을 크게 미치는 정도를 위쪽 층부터 아래쪽 층까지 back-tracking합니다."
                                    }
                                ]
                            }
                        ],
                        "style": "bullet",
                        "indent": 0,
                        "border": 0
                    },
                    {
                        "type": "rich_text_list",
                        "elements": [
                            {
                                "type": "rich_text_section",
                                "elements": [
                                    {
                                        "type": "text",
                                        "text": "이 논문은 이미 pruning 없이 훈련된 모델에 대해, 모델을 추가로 훈련하지는 않고 inference 속도만 높이는 것이 목적입니다."
                                    }
                                ]
                            },
                            {
                                "type": "rich_text_section",
                                "elements": [
                                    {
                                        "type": "text",
                                        "text": "위에서 언급한 '작은 모델'은 이미 있는 본 모델의 attention weight를 따라가도록 knowledge distillation으로 훈련됩니다."
                                    }
                                ]
                            },
                            {
                                "type": "rich_text_section",
                                "elements": [
                                    {
                                        "type": "text",
                                        "text": "CV(image classification)에서 20%  이하, NLP(GLUE)에서 10% 이하의 token들만 사용해도 성능이 크게 안 떨어진다고 합니다."
                                    }
                                ]
                            }
                        ],
                        "style": "bullet",
                        "indent": 1,
                        "border": 0
                    },
                    {
                        "type": "rich_text_list",
                        "elements": [
                            {
                                "type": "rich_text_section",
                                "elements": [
                                    {
                                        "type": "text",
                                        "text": "의견: 실제로 얼마나 빨라지는지는 별개로, 기존 방법의 문제점을 잘 다룬 것 같습니다."
                                    }
                                ]
                            }
                        ],
                        "style": "bullet",
                        "indent": 0,
                        "border": 0
                    },
                    {
                        "type": "rich_text_list",
                        "elements": [
                            {
                                "type": "rich_text_section",
                                "elements": [
                                    {
                                        "type": "text",
                                        "text": "모든 token에서 output을 내 놔야 하는 문제에 대해서는 token pruning이 깔끔한 방책을 못 내고 있는 것 같습니다. 위의 Length-Adaptive Transformer가 그나마 해결책을 내긴 합니다."
                                    }
                                ]
                            },
                            {
                                "type": "rich_text_section",
                                "elements": [
                                    {
                                        "type": "text",
                                        "text": "GPT와 같은 auto-regressive Transformer에서도 이런 token purning이 사용될 수 있을지 보는 것도 흥미로운 주제입니다."
                                    }
                                ]
                            },
                            {
                                "type": "rich_text_section",
                                "elements": [
                                    {
                                        "type": "text",
                                        "text": "최종 output에 각 token이 얼마나 기여하는지를 알아내고자 한다는 점에서, 목적은 다르지만 attention map visualization과도 접점이 있을 것 같습니다."
                                    }
                                ]
                            }
                        ],
                        "style": "bullet",
                        "indent": 1,
                        "border": 0
                    }
                ]
            }
        ],
        "original": {
            "client_msg_id": "0a49456a-3d2d-46d7-bccd-312376db5b3d",
            "type": "message",
            "text": "Transformer 모델의 속도를 높이기 위해, 덜 중요한 token들을 제거해 나가면서 추론하는 방법(a.k.a. token pruning)이 활발히 연구되고 있습니다. 이 중 이번 ICLR에 발표될 논문입니다:\n<https:\/\/openreview.net\/forum?id=VV0hSE8AxCw|Sparse Token Transformers with Attention Back Tracking (ICLR 2023)>\n• 기존 방법: 이전 층에서 attention weight가 작았던 token들을 제거하고 남은 token들로 다음 층 연산을 수행합니다. \n    ◦ 이런 접근 방식은 최종 output에 미칠 영향을 전체적으로 고려하지 못한다는 단점이 있고, 몇 개나\/어떤 기준으로 제거해야 하는지가 연구의 핵심이었습니다.\n    ◦ <https:\/\/arxiv.org\/abs\/2001.08950|PoWER-BERT>, <https:\/\/arxiv.org\/abs\/2010.07003|Length-Adaptive Transformer>, <https:\/\/arxiv.org\/abs\/2012.09852|SpAtten>, <https:\/\/arxiv.org\/abs\/2107.00910|Learned Token Pruning>, <https:\/\/arxiv.org\/abs\/2210.05832|SaiT>\n• 핵심 아이디어: 모든 층의 attention weight를 예측하는 별도의 작은 모델을 훈련하고, 그렇게 찾은 attention weight들을 사용해 최종 token [CLS]에 영향을 크게 미치는 정도를 위쪽 층부터 아래쪽 층까지 back-tracking합니다.\n    ◦ 이 논문은 이미 pruning 없이 훈련된 모델에 대해, 모델을 추가로 훈련하지는 않고 inference 속도만 높이는 것이 목적입니다.\n    ◦ 위에서 언급한 '작은 모델'은 이미 있는 본 모델의 attention weight를 따라가도록 knowledge distillation으로 훈련됩니다.\n    ◦ CV(image classification)에서 20%  이하, NLP(GLUE)에서 10% 이하의 token들만 사용해도 성능이 크게 안 떨어진다고 합니다.\n• 의견: 실제로 얼마나 빨라지는지는 별개로, 기존 방법의 문제점을 잘 다룬 것 같습니다.\n    ◦ 모든 token에서 output을 내 놔야 하는 문제에 대해서는 token pruning이 깔끔한 방책을 못 내고 있는 것 같습니다. 위의 Length-Adaptive Transformer가 그나마 해결책을 내긴 합니다.\n    ◦ GPT와 같은 auto-regressive Transformer에서도 이런 token purning이 사용될 수 있을지 보는 것도 흥미로운 주제입니다.\n    ◦ 최종 output에 각 token이 얼마나 기여하는지를 알아내고자 한다는 점에서, 목적은 다르지만 attention map visualization과도 접점이 있을 것 같습니다.",
            "user": "U04MQ2J38JC",
            "ts": "1676212879.655989",
            "blocks": [
                {
                    "type": "rich_text",
                    "block_id": "wmdQ",
                    "elements": [
                        {
                            "type": "rich_text_section",
                            "elements": [
                                {
                                    "type": "text",
                                    "text": "Transformer 모델의 속도를 높이기 위해, 덜 중요한 token들을 제거해 나가면서 추론하는 방법(a.k.a. token pruning)이 활발히 연구되고 있습니다. 이 중 이번 ICLR에 발표될 논문입니다:\n"
                                },
                                {
                                    "type": "link",
                                    "url": "https:\/\/openreview.net\/forum?id=VV0hSE8AxCw",
                                    "text": "Sparse Token Transformers with Attention Back Tracking (ICLR 2023)"
                                },
                                {
                                    "type": "text",
                                    "text": "\n"
                                }
                            ]
                        },
                        {
                            "type": "rich_text_list",
                            "elements": [
                                {
                                    "type": "rich_text_section",
                                    "elements": [
                                        {
                                            "type": "text",
                                            "text": "기존 방법: 이전 층에서 attention weight가 작았던 token들을 제거하고 남은 token들로 다음 층 연산을 수행합니다. "
                                        }
                                    ]
                                }
                            ],
                            "style": "bullet",
                            "indent": 0,
                            "border": 0
                        },
                        {
                            "type": "rich_text_list",
                            "elements": [
                                {
                                    "type": "rich_text_section",
                                    "elements": [
                                        {
                                            "type": "text",
                                            "text": "이런 접근 방식은 최종 output에 미칠 영향을 전체적으로 고려하지 못한다는 단점이 있고, 몇 개나\/어떤 기준으로 제거해야 하는지가 연구의 핵심이었습니다."
                                        }
                                    ]
                                },
                                {
                                    "type": "rich_text_section",
                                    "elements": [
                                        {
                                            "type": "link",
                                            "url": "https:\/\/arxiv.org\/abs\/2001.08950",
                                            "text": "PoWER-BERT"
                                        },
                                        {
                                            "type": "text",
                                            "text": ", "
                                        },
                                        {
                                            "type": "link",
                                            "url": "https:\/\/arxiv.org\/abs\/2010.07003",
                                            "text": "Length-Adaptive Transformer"
                                        },
                                        {
                                            "type": "text",
                                            "text": ", "
                                        },
                                        {
                                            "type": "link",
                                            "url": "https:\/\/arxiv.org\/abs\/2012.09852",
                                            "text": "SpAtten"
                                        },
                                        {
                                            "type": "text",
                                            "text": ", "
                                        },
                                        {
                                            "type": "link",
                                            "url": "https:\/\/arxiv.org\/abs\/2107.00910",
                                            "text": "Learned Token Pruning"
                                        },
                                        {
                                            "type": "text",
                                            "text": ", "
                                        },
                                        {
                                            "type": "link",
                                            "url": "https:\/\/arxiv.org\/abs\/2210.05832",
                                            "text": "SaiT"
                                        }
                                    ]
                                }
                            ],
                            "style": "bullet",
                            "indent": 1,
                            "border": 0
                        },
                        {
                            "type": "rich_text_list",
                            "elements": [
                                {
                                    "type": "rich_text_section",
                                    "elements": [
                                        {
                                            "type": "text",
                                            "text": "핵심 아이디어: 모든 층의 attention weight를 예측하는 별도의 작은 모델을 훈련하고, 그렇게 찾은 attention weight들을 사용해 최종 token [CLS]에 영향을 크게 미치는 정도를 위쪽 층부터 아래쪽 층까지 back-tracking합니다."
                                        }
                                    ]
                                }
                            ],
                            "style": "bullet",
                            "indent": 0,
                            "border": 0
                        },
                        {
                            "type": "rich_text_list",
                            "elements": [
                                {
                                    "type": "rich_text_section",
                                    "elements": [
                                        {
                                            "type": "text",
                                            "text": "이 논문은 이미 pruning 없이 훈련된 모델에 대해, 모델을 추가로 훈련하지는 않고 inference 속도만 높이는 것이 목적입니다."
                                        }
                                    ]
                                },
                                {
                                    "type": "rich_text_section",
                                    "elements": [
                                        {
                                            "type": "text",
                                            "text": "위에서 언급한 '작은 모델'은 이미 있는 본 모델의 attention weight를 따라가도록 knowledge distillation으로 훈련됩니다."
                                        }
                                    ]
                                },
                                {
                                    "type": "rich_text_section",
                                    "elements": [
                                        {
                                            "type": "text",
                                            "text": "CV(image classification)에서 20%  이하, NLP(GLUE)에서 10% 이하의 token들만 사용해도 성능이 크게 안 떨어진다고 합니다."
                                        }
                                    ]
                                }
                            ],
                            "style": "bullet",
                            "indent": 1,
                            "border": 0
                        },
                        {
                            "type": "rich_text_list",
                            "elements": [
                                {
                                    "type": "rich_text_section",
                                    "elements": [
                                        {
                                            "type": "text",
                                            "text": "의견: 실제로 얼마나 빨라지는지는 별개로, 기존 방법의 문제점을 잘 다룬 것 같습니다."
                                        }
                                    ]
                                }
                            ],
                            "style": "bullet",
                            "indent": 0,
                            "border": 0
                        },
                        {
                            "type": "rich_text_list",
                            "elements": [
                                {
                                    "type": "rich_text_section",
                                    "elements": [
                                        {
                                            "type": "text",
                                            "text": "모든 token에서 output을 내 놔야 하는 문제에 대해서는 token pruning이 깔끔한 방책을 못 내고 있는 것 같습니다. 위의 Length-Adaptive Transformer가 그나마 해결책을 내긴 합니다."
                                        }
                                    ]
                                },
                                {
                                    "type": "rich_text_section",
                                    "elements": [
                                        {
                                            "type": "text",
                                            "text": "GPT와 같은 auto-regressive Transformer에서도 이런 token purning이 사용될 수 있을지 보는 것도 흥미로운 주제입니다."
                                        }
                                    ]
                                },
                                {
                                    "type": "rich_text_section",
                                    "elements": [
                                        {
                                            "type": "text",
                                            "text": "최종 output에 각 token이 얼마나 기여하는지를 알아내고자 한다는 점에서, 목적은 다르지만 attention map visualization과도 접점이 있을 것 같습니다."
                                        }
                                    ]
                                }
                            ],
                            "style": "bullet",
                            "indent": 1,
                            "border": 0
                        }
                    ]
                }
            ],
            "team": "T04MCQMEXQ9",
            "user_team": "T04MCQMEXQ9",
            "source_team": "T04MCQMEXQ9",
            "user_profile": {
                "avatar_hash": "3e494054f705",
                "image_72": "https:\/\/avatars.slack-edge.com\/2023-01-28\/4714393981186_3e494054f7050068883f_72.png",
                "first_name": "심규홍\/퀄컴",
                "real_name": "심규홍\/퀄컴",
                "display_name": "",
                "team": "T04MCQMEXQ9",
                "name": "khshim20",
                "is_restricted": false,
                "is_ultra_restricted": false
            },
            "edited": {
                "user": "U04MQ2J38JC",
                "ts": "1676212994.000000"
            }
        },
        "edited_by": "U04MQ2J38JC"
    },
    {
        "type": "message",
        "user": "U04MQ2J38JC",
        "upload": false,
        "ts": "1676212994.000000",
        "text": "Transformer 모델의 속도를 높이기 위해, 덜 중요한 token들을 제거해 나가면서 추론하는 방법(a.k.a. token pruning)이 활발히 연구되고 있습니다. 이 중 이번 ICLR에 발표될 논문입니다:\n<https:\/\/openreview.net\/forum?id=VV0hSE8AxCw|Sparse Token Transformers with Attention Back Tracking (ICLR 2023)>\n• 기존 방법: 이전 층에서 attention weight가 작았던 token들을 제거하고 남은 token들로 다음 층 연산을 수행합니다. \n    ◦ 이런 접근 방식은 최종 output에 미칠 영향을 전체적으로 고려하지 못한다는 단점이 있고, 몇 개나\/어떤 기준으로 제거해야 하는지가 연구의 중요한 부분이었습니다.\n    ◦ <https:\/\/arxiv.org\/abs\/2001.08950|PoWER-BERT>, <https:\/\/arxiv.org\/abs\/2010.07003|Length-Adaptive Transformer>, <https:\/\/arxiv.org\/abs\/2012.09852|SpAtten>, <https:\/\/arxiv.org\/abs\/2107.00910|Learned Token Pruning>, <https:\/\/arxiv.org\/abs\/2210.05832|SaiT>\n• 핵심 아이디어: 모든 층의 attention weight를 예측하는 별도의 작은 모델을 훈련하고, 그렇게 찾은 attention weight들을 사용해 최종 token [CLS]에 영향을 크게 미치는 정도를 위쪽 층부터 아래쪽 층까지 back-tracking합니다.\n    ◦ 이 논문은 이미 pruning 없이 훈련된 모델에 대해, 모델을 추가로 훈련하지는 않고 inference 속도만 높이는 것이 목적입니다.\n    ◦ 위에서 언급한 '작은 모델'은 이미 있는 본 모델의 attention weight를 따라가도록 knowledge distillation으로 훈련됩니다.\n    ◦ CV(image classification)에서 20%  이하, NLP(GLUE)에서 10% 이하의 token들만 사용해도 성능이 크게 안 떨어진다고 합니다.\n• 의견: 실제로 얼마나 빨라지는지는 별개로, 기존 방법의 문제점을 잘 다룬 것 같습니다.\n    ◦ 모든 token에서 output을 내 놔야 하는 문제에 대해서는 token pruning이 깔끔한 방책을 못 내고 있는 것 같습니다. 위의 Length-Adaptive Transformer가 그나마 해결책을 내긴 합니다.\n    ◦ GPT와 같은 auto-regressive Transformer에서도 이런 token purning이 사용될 수 있을지 보는 것도 흥미로운 주제입니다.\n    ◦ 최종 output에 각 token이 얼마나 기여하는지를 알아내고자 한다는 점에서, 목적은 다르지만 attention map visualization과도 접점이 있을 것 같습니다.",
        "subtype": "message_changed",
        "editor_id": "U04MQ2J38JC",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "MlUS",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "Transformer 모델의 속도를 높이기 위해, 덜 중요한 token들을 제거해 나가면서 추론하는 방법(a.k.a. token pruning)이 활발히 연구되고 있습니다. 이 중 이번 ICLR에 발표될 논문입니다:\n"
                            },
                            {
                                "type": "link",
                                "url": "https:\/\/openreview.net\/forum?id=VV0hSE8AxCw",
                                "text": "Sparse Token Transformers with Attention Back Tracking (ICLR 2023)"
                            },
                            {
                                "type": "text",
                                "text": "\n"
                            }
                        ]
                    },
                    {
                        "type": "rich_text_list",
                        "elements": [
                            {
                                "type": "rich_text_section",
                                "elements": [
                                    {
                                        "type": "text",
                                        "text": "기존 방법: 이전 층에서 attention weight가 작았던 token들을 제거하고 남은 token들로 다음 층 연산을 수행합니다. "
                                    }
                                ]
                            }
                        ],
                        "style": "bullet",
                        "indent": 0,
                        "border": 0
                    },
                    {
                        "type": "rich_text_list",
                        "elements": [
                            {
                                "type": "rich_text_section",
                                "elements": [
                                    {
                                        "type": "text",
                                        "text": "이런 접근 방식은 최종 output에 미칠 영향을 전체적으로 고려하지 못한다는 단점이 있고, 몇 개나\/어떤 기준으로 제거해야 하는지가 연구의 중요한 부분이었습니다."
                                    }
                                ]
                            },
                            {
                                "type": "rich_text_section",
                                "elements": [
                                    {
                                        "type": "link",
                                        "url": "https:\/\/arxiv.org\/abs\/2001.08950",
                                        "text": "PoWER-BERT"
                                    },
                                    {
                                        "type": "text",
                                        "text": ", "
                                    },
                                    {
                                        "type": "link",
                                        "url": "https:\/\/arxiv.org\/abs\/2010.07003",
                                        "text": "Length-Adaptive Transformer"
                                    },
                                    {
                                        "type": "text",
                                        "text": ", "
                                    },
                                    {
                                        "type": "link",
                                        "url": "https:\/\/arxiv.org\/abs\/2012.09852",
                                        "text": "SpAtten"
                                    },
                                    {
                                        "type": "text",
                                        "text": ", "
                                    },
                                    {
                                        "type": "link",
                                        "url": "https:\/\/arxiv.org\/abs\/2107.00910",
                                        "text": "Learned Token Pruning"
                                    },
                                    {
                                        "type": "text",
                                        "text": ", "
                                    },
                                    {
                                        "type": "link",
                                        "url": "https:\/\/arxiv.org\/abs\/2210.05832",
                                        "text": "SaiT"
                                    }
                                ]
                            }
                        ],
                        "style": "bullet",
                        "indent": 1,
                        "border": 0
                    },
                    {
                        "type": "rich_text_list",
                        "elements": [
                            {
                                "type": "rich_text_section",
                                "elements": [
                                    {
                                        "type": "text",
                                        "text": "핵심 아이디어: 모든 층의 attention weight를 예측하는 별도의 작은 모델을 훈련하고, 그렇게 찾은 attention weight들을 사용해 최종 token [CLS]에 영향을 크게 미치는 정도를 위쪽 층부터 아래쪽 층까지 back-tracking합니다."
                                    }
                                ]
                            }
                        ],
                        "style": "bullet",
                        "indent": 0,
                        "border": 0
                    },
                    {
                        "type": "rich_text_list",
                        "elements": [
                            {
                                "type": "rich_text_section",
                                "elements": [
                                    {
                                        "type": "text",
                                        "text": "이 논문은 이미 pruning 없이 훈련된 모델에 대해, 모델을 추가로 훈련하지는 않고 inference 속도만 높이는 것이 목적입니다."
                                    }
                                ]
                            },
                            {
                                "type": "rich_text_section",
                                "elements": [
                                    {
                                        "type": "text",
                                        "text": "위에서 언급한 '작은 모델'은 이미 있는 본 모델의 attention weight를 따라가도록 knowledge distillation으로 훈련됩니다."
                                    }
                                ]
                            },
                            {
                                "type": "rich_text_section",
                                "elements": [
                                    {
                                        "type": "text",
                                        "text": "CV(image classification)에서 20%  이하, NLP(GLUE)에서 10% 이하의 token들만 사용해도 성능이 크게 안 떨어진다고 합니다."
                                    }
                                ]
                            }
                        ],
                        "style": "bullet",
                        "indent": 1,
                        "border": 0
                    },
                    {
                        "type": "rich_text_list",
                        "elements": [
                            {
                                "type": "rich_text_section",
                                "elements": [
                                    {
                                        "type": "text",
                                        "text": "의견: 실제로 얼마나 빨라지는지는 별개로, 기존 방법의 문제점을 잘 다룬 것 같습니다."
                                    }
                                ]
                            }
                        ],
                        "style": "bullet",
                        "indent": 0,
                        "border": 0
                    },
                    {
                        "type": "rich_text_list",
                        "elements": [
                            {
                                "type": "rich_text_section",
                                "elements": [
                                    {
                                        "type": "text",
                                        "text": "모든 token에서 output을 내 놔야 하는 문제에 대해서는 token pruning이 깔끔한 방책을 못 내고 있는 것 같습니다. 위의 Length-Adaptive Transformer가 그나마 해결책을 내긴 합니다."
                                    }
                                ]
                            },
                            {
                                "type": "rich_text_section",
                                "elements": [
                                    {
                                        "type": "text",
                                        "text": "GPT와 같은 auto-regressive Transformer에서도 이런 token purning이 사용될 수 있을지 보는 것도 흥미로운 주제입니다."
                                    }
                                ]
                            },
                            {
                                "type": "rich_text_section",
                                "elements": [
                                    {
                                        "type": "text",
                                        "text": "최종 output에 각 token이 얼마나 기여하는지를 알아내고자 한다는 점에서, 목적은 다르지만 attention map visualization과도 접점이 있을 것 같습니다."
                                    }
                                ]
                            }
                        ],
                        "style": "bullet",
                        "indent": 1,
                        "border": 0
                    }
                ]
            }
        ],
        "original": {
            "client_msg_id": "0a49456a-3d2d-46d7-bccd-312376db5b3d",
            "type": "message",
            "text": "Transformer 모델의 속도를 높이기 위해, 덜 중요한 token들을 제거해 나가면서 추론하는 방법(a.k.a. token pruning)이 활발히 연구되고 있습니다. 이 중 이번 ICLR에 발표될 논문입니다:\n<https:\/\/openreview.net\/forum?id=VV0hSE8AxCw|Sparse Token Transformers with Attention Back Tracking (ICLR 2023)>\n• 기존 방법: 이전 층에서 attention weight가 작았던 token들을 제거하고 남은 token들로 다음 층 연산을 수행합니다. \n    ◦ 이런 접근 방식은 최종 output에 미칠 영향을 전체적으로 고려하지 못한다는 단점이 있고, 몇 개나 어떤 기준으로 제거해야 하는지가 연구의 핵심이었습니다.\n    ◦ <https:\/\/arxiv.org\/abs\/2001.08950|PoWER-BERT>, <https:\/\/arxiv.org\/abs\/2010.07003|Length-Adaptive Transformer>, <https:\/\/arxiv.org\/abs\/2012.09852|SpAtten>, <https:\/\/arxiv.org\/abs\/2107.00910|Learned Token Pruning>, <https:\/\/arxiv.org\/abs\/2210.05832|SaiT>\n• 핵심 아이디어: 모든 층의 attention weight를 예측하는 별도의 작은 모델을 훈련하고, 그렇게 찾은 attention weight들을 사용해 최종 token [CLS]에 영향을 크게 미치는 정도를 위쪽 층부터 아래쪽 층까지 back-tracking합니다.\n    ◦ 이 방법은 이미 pruning 없이 훈련된 모델에 대해, 모델을 추가로 훈련하지는 않고 inference 속도만 높이는 것이 목적입니다.\n    ◦ 위에서 언급한 '작은 모델'은 이미 있는 본 모델의 attention weight를 따라가도록 knowledge distillation으로 훈련됩니다.\n    ◦ CV(image classification)에서 20%  이하, NLP(GLUE)에서 10% 이하의 token들만 사용해도 성능이 크게 안 떨어진다고 합니다.\n• 의견: 실제로 얼마나 빨라지는지는 별개로, 기존 방법의 문제점을 잘 다룬 것 같습니다.\n    ◦ 모든 token에서 output을 내 놔야 하는 문제에 대해서는 token pruning이 깔끔한 방책을 못 내고 있는 것 같습니다. 위의 Length-Adaptive Transformer가 그나마 해결책을 내긴 합니다.\n    ◦ GPT와 같은 auto-regressive Transformer에서도 이런 token purning이 사용될 수 있을지 보는 것도 흥미로운 주제일 것 같습니다.",
            "user": "U04MQ2J38JC",
            "ts": "1676212879.655989",
            "blocks": [
                {
                    "type": "rich_text",
                    "block_id": "j6UFL",
                    "elements": [
                        {
                            "type": "rich_text_section",
                            "elements": [
                                {
                                    "type": "text",
                                    "text": "Transformer 모델의 속도를 높이기 위해, 덜 중요한 token들을 제거해 나가면서 추론하는 방법(a.k.a. token pruning)이 활발히 연구되고 있습니다. 이 중 이번 ICLR에 발표될 논문입니다:\n"
                                },
                                {
                                    "type": "link",
                                    "url": "https:\/\/openreview.net\/forum?id=VV0hSE8AxCw",
                                    "text": "Sparse Token Transformers with Attention Back Tracking (ICLR 2023)"
                                },
                                {
                                    "type": "text",
                                    "text": "\n"
                                }
                            ]
                        },
                        {
                            "type": "rich_text_list",
                            "elements": [
                                {
                                    "type": "rich_text_section",
                                    "elements": [
                                        {
                                            "type": "text",
                                            "text": "기존 방법: 이전 층에서 attention weight가 작았던 token들을 제거하고 남은 token들로 다음 층 연산을 수행합니다. "
                                        }
                                    ]
                                }
                            ],
                            "style": "bullet",
                            "indent": 0,
                            "border": 0
                        },
                        {
                            "type": "rich_text_list",
                            "elements": [
                                {
                                    "type": "rich_text_section",
                                    "elements": [
                                        {
                                            "type": "text",
                                            "text": "이런 접근 방식은 최종 output에 미칠 영향을 전체적으로 고려하지 못한다는 단점이 있고, 몇 개나 어떤 기준으로 제거해야 하는지가 연구의 핵심이었습니다."
                                        }
                                    ]
                                },
                                {
                                    "type": "rich_text_section",
                                    "elements": [
                                        {
                                            "type": "link",
                                            "url": "https:\/\/arxiv.org\/abs\/2001.08950",
                                            "text": "PoWER-BERT"
                                        },
                                        {
                                            "type": "text",
                                            "text": ", "
                                        },
                                        {
                                            "type": "link",
                                            "url": "https:\/\/arxiv.org\/abs\/2010.07003",
                                            "text": "Length-Adaptive Transformer"
                                        },
                                        {
                                            "type": "text",
                                            "text": ", "
                                        },
                                        {
                                            "type": "link",
                                            "url": "https:\/\/arxiv.org\/abs\/2012.09852",
                                            "text": "SpAtten"
                                        },
                                        {
                                            "type": "text",
                                            "text": ", "
                                        },
                                        {
                                            "type": "link",
                                            "url": "https:\/\/arxiv.org\/abs\/2107.00910",
                                            "text": "Learned Token Pruning"
                                        },
                                        {
                                            "type": "text",
                                            "text": ", "
                                        },
                                        {
                                            "type": "link",
                                            "url": "https:\/\/arxiv.org\/abs\/2210.05832",
                                            "text": "SaiT"
                                        }
                                    ]
                                }
                            ],
                            "style": "bullet",
                            "indent": 1,
                            "border": 0
                        },
                        {
                            "type": "rich_text_list",
                            "elements": [
                                {
                                    "type": "rich_text_section",
                                    "elements": [
                                        {
                                            "type": "text",
                                            "text": "핵심 아이디어: 모든 층의 attention weight를 예측하는 별도의 작은 모델을 훈련하고, 그렇게 찾은 attention weight들을 사용해 최종 token [CLS]에 영향을 크게 미치는 정도를 위쪽 층부터 아래쪽 층까지 back-tracking합니다."
                                        }
                                    ]
                                }
                            ],
                            "style": "bullet",
                            "indent": 0,
                            "border": 0
                        },
                        {
                            "type": "rich_text_list",
                            "elements": [
                                {
                                    "type": "rich_text_section",
                                    "elements": [
                                        {
                                            "type": "text",
                                            "text": "이 방법은 이미 pruning 없이 훈련된 모델에 대해, 모델을 추가로 훈련하지는 않고 inference 속도만 높이는 것이 목적입니다."
                                        }
                                    ]
                                },
                                {
                                    "type": "rich_text_section",
                                    "elements": [
                                        {
                                            "type": "text",
                                            "text": "위에서 언급한 '작은 모델'은 이미 있는 본 모델의 attention weight를 따라가도록 knowledge distillation으로 훈련됩니다."
                                        }
                                    ]
                                },
                                {
                                    "type": "rich_text_section",
                                    "elements": [
                                        {
                                            "type": "text",
                                            "text": "CV(image classification)에서 20%  이하, NLP(GLUE)에서 10% 이하의 token들만 사용해도 성능이 크게 안 떨어진다고 합니다."
                                        }
                                    ]
                                }
                            ],
                            "style": "bullet",
                            "indent": 1,
                            "border": 0
                        },
                        {
                            "type": "rich_text_list",
                            "elements": [
                                {
                                    "type": "rich_text_section",
                                    "elements": [
                                        {
                                            "type": "text",
                                            "text": "의견: 실제로 얼마나 빨라지는지는 별개로, 기존 방법의 문제점을 잘 다룬 것 같습니다."
                                        }
                                    ]
                                }
                            ],
                            "style": "bullet",
                            "indent": 0,
                            "border": 0
                        },
                        {
                            "type": "rich_text_list",
                            "elements": [
                                {
                                    "type": "rich_text_section",
                                    "elements": [
                                        {
                                            "type": "text",
                                            "text": "모든 token에서 output을 내 놔야 하는 문제에 대해서는 token pruning이 깔끔한 방책을 못 내고 있는 것 같습니다. 위의 Length-Adaptive Transformer가 그나마 해결책을 내긴 합니다."
                                        }
                                    ]
                                },
                                {
                                    "type": "rich_text_section",
                                    "elements": [
                                        {
                                            "type": "text",
                                            "text": "GPT와 같은 auto-regressive Transformer에서도 이런 token purning이 사용될 수 있을지 보는 것도 흥미로운 주제일 것 같습니다."
                                        }
                                    ]
                                }
                            ],
                            "style": "bullet",
                            "indent": 1,
                            "border": 0
                        }
                    ]
                }
            ],
            "team": "T04MCQMEXQ9",
            "user_team": "T04MCQMEXQ9",
            "source_team": "T04MCQMEXQ9",
            "user_profile": {
                "avatar_hash": "3e494054f705",
                "image_72": "https:\/\/avatars.slack-edge.com\/2023-01-28\/4714393981186_3e494054f7050068883f_72.png",
                "first_name": "심규홍\/퀄컴",
                "real_name": "심규홍\/퀄컴",
                "display_name": "",
                "team": "T04MCQMEXQ9",
                "name": "khshim20",
                "is_restricted": false,
                "is_ultra_restricted": false
            }
        },
        "edited_by": "U04MQ2J38JC"
    }
]