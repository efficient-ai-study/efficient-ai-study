[
    {
        "client_msg_id": "3d23b1c5-115f-4837-8667-3fe9e93da577",
        "type": "message",
        "text": "안녕하세요. quantization 써주신 글들을 보다가 질문이 생겨서 질문드립니다.! (초보적인 질문일 수 있습니다:smiling_face_with_tear:)\n\n1. 성능이슈 제외하고 activation과 weight를 둘다 int4로 압축하지 않을 이유가 있을까요? 매 activation마다 quantize 해야하는 과정이 오버헤드로 작용하는걸까요?\n2. Quantization 에서도 pruning 논문과 비슷하게 layer 별로  sparsity (int4, int8, fp16, fp32.. ) 를 다르게 학습 할 수 있을 것 같은데 이럴 경우에는 dequantize와 quantize를 반복하는 과정이 오버헤드로 작용할까요? 연구나 현업에서도 많이 쓰이는 방법인가요?",
        "user": "U04LL3W56F9",
        "ts": "1675413310.805959",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "0EROE",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "안녕하세요. quantization 써주신 글들을 보다가 질문이 생겨서 질문드립니다.! (초보적인 질문일 수 있습니다"
                            },
                            {
                                "type": "emoji",
                                "name": "smiling_face_with_tear",
                                "unicode": "1f972"
                            },
                            {
                                "type": "text",
                                "text": ")\n\n"
                            }
                        ]
                    },
                    {
                        "type": "rich_text_list",
                        "elements": [
                            {
                                "type": "rich_text_section",
                                "elements": [
                                    {
                                        "type": "text",
                                        "text": "성능이슈 제외하고 activation과 weight를 둘다 int4로 압축하지 않을 이유가 있을까요? 매 activation마다 quantize 해야하는 과정이 오버헤드로 작용하는걸까요?"
                                    }
                                ]
                            },
                            {
                                "type": "rich_text_section",
                                "elements": [
                                    {
                                        "type": "text",
                                        "text": "Quantization 에서도 pruning 논문과 비슷하게 layer 별로  sparsity (int4, int8, fp16, fp32.. ) 를 다르게 학습 할 수 있을 것 같은데 이럴 경우에는 dequantize와 quantize를 반복하는 과정이 오버헤드로 작용할까요? 연구나 현업에서도 많이 쓰이는 방법인가요?"
                                    }
                                ]
                            }
                        ],
                        "style": "ordered",
                        "indent": 0,
                        "border": 0
                    }
                ]
            }
        ],
        "team": "T04MCQMEXQ9",
        "user_team": "T04MCQMEXQ9",
        "source_team": "T04MCQMEXQ9",
        "user_profile": {
            "avatar_hash": "da9e9b936e49",
            "image_72": "https:\/\/avatars.slack-edge.com\/2023-01-29\/4727308105281_da9e9b936e4974fcbfe1_72.png",
            "first_name": "이광한",
            "real_name": "이광한",
            "display_name": "이광한\/성균관대",
            "team": "T04MCQMEXQ9",
            "name": "ican0016",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "thread_ts": "1675336960.897349",
        "parent_user_id": "U04M2NY6U2Y"
    },
    {
        "client_msg_id": "cc32f9ef-70cb-4720-9cae-8f64df4d7d06",
        "type": "message",
        "text": "1. 성능 이슈때문에 안하는거구요. 지금도 LLM activation int8은 굉장히 aggressive합니다. LLM.int8() 논문을 참고해주세요. smoothquant\/llm.int8 논문 모두 outlier의 systematic한 emergence가 아니면 성립하지 않습니다. \n2. 그럴수 있는데, 거기까지 파내는걸 굳이 논문레벨에서 하는게 의미 없는게 크구요. 사실 Transformer는 중간에 softmax때문에라도 quant\/dequant를 반복해야합니다. 그게 오버헤드이긴 하지만, layer별로 다르게 지정하지 못하는 이유가 되지는 못합니다. fp32는 별 의미 없을것 같고, fp16\/int8하는건 최근에 nvidia가 mixed precision transformer training 얘기한거 같구요. int4는 aggressive하구요.",
        "user": "U04M05UHX7U",
        "ts": "1675419648.433509",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "\/NAt",
                "elements": [
                    {
                        "type": "rich_text_list",
                        "elements": [
                            {
                                "type": "rich_text_section",
                                "elements": [
                                    {
                                        "type": "text",
                                        "text": "성능 이슈때문에 안하는거구요. 지금도 LLM activation int8은 굉장히 aggressive합니다. LLM.int8() 논문을 참고해주세요. smoothquant\/llm.int8 논문 모두 outlier의 systematic한 emergence가 아니면 성립하지 않습니다. "
                                    }
                                ]
                            },
                            {
                                "type": "rich_text_section",
                                "elements": [
                                    {
                                        "type": "text",
                                        "text": "그럴수 있는데, 거기까지 파내는걸 굳이 논문레벨에서 하는게 의미 없는게 크구요. 사실 Transformer는 중간에 softmax때문에라도 quant\/dequant를 반복해야합니다. 그게 오버헤드이긴 하지만, layer별로 다르게 지정하지 못하는 이유가 되지는 못합니다. fp32는 별 의미 없을것 같고, fp16\/int8하는건 최근에 nvidia가 mixed precision transformer training 얘기한거 같구요. int4는 aggressive하구요."
                                    }
                                ]
                            }
                        ],
                        "style": "ordered",
                        "indent": 0,
                        "border": 0
                    }
                ]
            }
        ],
        "team": "T04MCQMEXQ9",
        "user_team": "T04MCQMEXQ9",
        "source_team": "T04MCQMEXQ9",
        "user_profile": {
            "avatar_hash": "8160001f5214",
            "image_72": "https:\/\/avatars.slack-edge.com\/2023-01-29\/4699744492023_8160001f52148d13414d_72.jpg",
            "first_name": "권세중\/네이버클라우드",
            "real_name": "권세중\/네이버클라우드",
            "display_name": "권세중\/네이버클라우드",
            "team": "T04MCQMEXQ9",
            "name": "sejung.kwon",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "thread_ts": "1675336960.897349",
        "parent_user_id": "U04M2NY6U2Y"
    }
]