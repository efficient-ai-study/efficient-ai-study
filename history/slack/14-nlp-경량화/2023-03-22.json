[
    {
        "client_msg_id": "2565569d-d58e-4fb0-84fc-a68cb81b1af2",
        "type": "message",
        "text": "LLM.int8() 저자의 Webinar 가 있어서 공유 드립니다. K-bit Inference Scaling Laws 의 몇 가지 추가 정보도 공유할 것이라고 합니다.\n\n한국 시간 : 3월 30일 오전 1시\nDettmers Twitter : <https:\/\/twitter.com\/Tim_Dettmers\/status\/1638250897414422528?s=20>\nRegistration : <https:\/\/zoom.us\/webinar\/register\/5416793494102\/WN_p521usmAQMOhb7wZLhcy5Q>\n\nAbstract : Large language models are effective tools for many tasks but are difficult to train and inference due to their size. Moving from 32-bit models to 16-bit models resulted in considerable efficiency gains that made training and inference of large models easier. Can we train and inference in 8-bit to make further gains? In this talk, Tim will show that 8-bit inference and training can be used without degrading performance while improving efficiency. To make 8-bit methods work, it is essential to understand how quantization precision affects model performance and training stability as we scale the model size. He will talk about how these factors change with scale and how we need to adjust 8-bit methods to make them work. In particular, he will speak about 8-bit optimizers for training and Int8 inference for large language models with up to 175B parameters. These methods make training and inference more efficient and make large models more accessible to researchers.",
        "user": "U04M2NY6U2Y",
        "ts": "1679540808.803729",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "fss",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "LLM.int8() 저자의 Webinar 가 있어서 공유 드립니다. K-bit Inference Scaling Laws 의 몇 가지 추가 정보도 공유할 것이라고 합니다.\n\n한국 시간 : 3월 30일 오전 1시\nDettmers Twitter : "
                            },
                            {
                                "type": "link",
                                "url": "https:\/\/twitter.com\/Tim_Dettmers\/status\/1638250897414422528?s=20"
                            },
                            {
                                "type": "text",
                                "text": "\nRegistration : "
                            },
                            {
                                "type": "link",
                                "url": "https:\/\/zoom.us\/webinar\/register\/5416793494102\/WN_p521usmAQMOhb7wZLhcy5Q"
                            },
                            {
                                "type": "text",
                                "text": "\n\nAbstract : Large language models are effective tools for many tasks but are difficult to train and inference due to their size. Moving from 32-bit models to 16-bit models resulted in considerable efficiency gains that made training and inference of large models easier. Can we train and inference in 8-bit to make further gains? In this talk, Tim will show that 8-bit inference and training can be used without degrading performance while improving efficiency. To make 8-bit methods work, it is essential to understand how quantization precision affects model performance and training stability as we scale the model size. He will talk about how these factors change with scale and how we need to adjust 8-bit methods to make them work. In particular, he will speak about 8-bit optimizers for training and Int8 inference for large language models with up to 175B parameters. These methods make training and inference more efficient and make large models more accessible to researchers."
                            }
                        ]
                    }
                ]
            }
        ],
        "team": "T04MCQMEXQ9",
        "user_team": "T04MCQMEXQ9",
        "source_team": "T04MCQMEXQ9",
        "user_profile": {
            "avatar_hash": "7026e9e9c4f8",
            "image_72": "https:\/\/avatars.slack-edge.com\/2023-01-29\/4699821662839_7026e9e9c4f8501ee814_72.png",
            "first_name": "김민수\/한양대",
            "real_name": "김민수\/한양대",
            "display_name": "김민수\/한양대",
            "team": "T04MCQMEXQ9",
            "name": "minsoo2333",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "thread_ts": "1679540808.803729",
        "reply_count": 2,
        "reply_users_count": 2,
        "latest_reply": "1680842921.099489",
        "reply_users": [
            "U04M091M3MZ",
            "U04M2NY6U2Y"
        ],
        "replies": [
            {
                "user": "U04M091M3MZ",
                "ts": "1680152465.796019"
            },
            {
                "user": "U04M2NY6U2Y",
                "ts": "1680842921.099489"
            }
        ],
        "is_locked": false,
        "subscribed": false,
        "reactions": [
            {
                "name": "+1",
                "users": [
                    "U04M0D54PEW",
                    "U04M091M3MZ",
                    "U04QJNZDDFB",
                    "U04MQ1B2BGQ",
                    "U04MQ2J38JC",
                    "U04LV2JHHB8",
                    "U04M059CL67",
                    "U04SJES55HS",
                    "U04M06CM95H",
                    "U04LTPY7LES",
                    "U04LTMVAQ4W",
                    "U04M0811ZPD",
                    "U04M05UHX7U",
                    "U04LXKFJLFP",
                    "U04LKU43D7Z",
                    "U04LU2NN8TY",
                    "U04TECY80AK",
                    "U04MQHDJ12L",
                    "U053HSVS3MM"
                ],
                "count": 19
            }
        ]
    }
]