[
    {
        "client_msg_id": "b8f9895a-7c31-4cba-a626-b0228acc2769",
        "type": "message",
        "text": "안녕하세요! NLP 경량화 관련해서 최근 ICLR 2023 에 Accept 된 GLB-130B 논문을 간단하게 리뷰해보려고 합니다. 새로운 Pre-Trained LLM 을 제시한 Technical Report 에 가까운 논문인데, 많은 시행착오 및 결과들을 Report 하고 있어서, LLM 을 경량화하는데 있어서 고민해볼만한 흥미로운 포인트들을 많이 건져볼 수 있을 것 같아서 가져왔습니다.\n<https:\/\/openreview.net\/forum?id=-Aw0rrrPUF>\n\n간단하게 Takeaways 만 정리하자면,\n• 기존 100B 이상급 Scale LLM (GPT, OPT, BLOOM) 보다 더 높은 성능을 기록하는 130B 급 LLM 모델을 제시했으며, Pre-Training 과정에서의 다양한 Challenge 들 및 Model Design Choice 에 대해 모두 공개하고 있습니다. (Model Download 는 Google Form 에 Application 에 대해 답변을 남기면 받아볼 수 있는 방식으로 되어있네요.)\n    ◦ <https:\/\/github.com\/THUDM\/GLM-130B>\n• Pre-Training 에서 발생하는 Training Instability 문제에 대해 DeepNorm, Embedding Gradient Scaling 방식을 사용하여 해결한 결과를 Empirical 하게 제시하고 있습니다.\n• (저는 이 부분이 가장 흥미로운데요..) GLM-130B 에서는 기존 LLM.int8 에서 제시한 Activation Outlier 가 30% 나 많이 발생하는 반면, (그래서 Activation 은 FP16 사용), Weight Distribution 이 기존 LLM 들 보다 훨씬 좁게 나타나, INT4 Weight Quantization 을 PTQ 로 성능 하락 없이 적용할 수 있다고 합니다. 따라서 Memory Saving 을 기존 LLM 압축 논문 보다 2배더 가져갈 수 있어 RTX-2080 (11GB) 8대에서도 Inference 가 가능하다고 강조하고 있습니다.\n좀 더 자세한 내용은 스레드에 달아두겠습니다. 정리가 많이 부족하겠지만, 여기 계신 많은 분들께 도움이 된다면 좋을 것 같고, 해당 논문에 대해 경량화 관점에서 어떻게 생각하시는지 함께 얘기해볼 수 있으면 좋을 것 같습니다.!",
        "user": "U04M2NY6U2Y",
        "ts": "1675336960.897349",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "98Omj",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "안녕하세요! NLP 경량화 관련해서 최근 ICLR 2023 에 Accept 된 GLB-130B 논문을 간단하게 리뷰해보려고 합니다. 새로운 Pre-Trained LLM 을 제시한 Technical Report 에 가까운 논문인데, 많은 시행착오 및 결과들을 Report 하고 있어서, LLM 을 경량화하는데 있어서 고민해볼만한 흥미로운 포인트들을 많이 건져볼 수 있을 것 같아서 가져왔습니다.\n"
                            },
                            {
                                "type": "link",
                                "url": "https:\/\/openreview.net\/forum?id=-Aw0rrrPUF"
                            },
                            {
                                "type": "text",
                                "text": "\n\n간단하게 Takeaways 만 정리하자면,\n"
                            }
                        ]
                    },
                    {
                        "type": "rich_text_list",
                        "elements": [
                            {
                                "type": "rich_text_section",
                                "elements": [
                                    {
                                        "type": "text",
                                        "text": "기존 100B 이상급 Scale LLM (GPT, OPT, BLOOM) 보다 더 높은 성능을 기록하는 130B 급 LLM 모델을 제시했으며, Pre-Training 과정에서의 다양한 Challenge 들 및 Model Design Choice 에 대해 모두 공개하고 있습니다. (Model Download 는 Google Form 에 Application 에 대해 답변을 남기면 받아볼 수 있는 방식으로 되어있네요.)"
                                    }
                                ]
                            }
                        ],
                        "style": "bullet",
                        "indent": 0,
                        "border": 0
                    },
                    {
                        "type": "rich_text_list",
                        "elements": [
                            {
                                "type": "rich_text_section",
                                "elements": [
                                    {
                                        "type": "link",
                                        "url": "https:\/\/github.com\/THUDM\/GLM-130B"
                                    }
                                ]
                            }
                        ],
                        "style": "bullet",
                        "indent": 1,
                        "border": 0
                    },
                    {
                        "type": "rich_text_list",
                        "elements": [
                            {
                                "type": "rich_text_section",
                                "elements": [
                                    {
                                        "type": "text",
                                        "text": "Pre-Training 에서 발생하는 Training Instability 문제에 대해 DeepNorm, Embedding Gradient Scaling 방식을 사용하여 해결한 결과를 Empirical 하게 제시하고 있습니다."
                                    }
                                ]
                            },
                            {
                                "type": "rich_text_section",
                                "elements": [
                                    {
                                        "type": "text",
                                        "text": "(저는 이 부분이 가장 흥미로운데요..) GLM-130B 에서는 기존 LLM.int8 에서 제시한 Activation Outlier 가 30% 나 많이 발생하는 반면, (그래서 Activation 은 FP16 사용), Weight Distribution 이 기존 LLM 들 보다 훨씬 좁게 나타나, INT4 Weight Quantization 을 PTQ 로 성능 하락 없이 적용할 수 있다고 합니다. 따라서 Memory Saving 을 기존 LLM 압축 논문 보다 2배더 가져갈 수 있어 RTX-2080 (11GB) 8대에서도 Inference 가 가능하다고 강조하고 있습니다."
                                    }
                                ]
                            }
                        ],
                        "style": "bullet",
                        "indent": 0,
                        "border": 0
                    },
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "\n좀 더 자세한 내용은 스레드에 달아두겠습니다. 정리가 많이 부족하겠지만, 여기 계신 많은 분들께 도움이 된다면 좋을 것 같고, 해당 논문에 대해 경량화 관점에서 어떻게 생각하시는지 함께 얘기해볼 수 있으면 좋을 것 같습니다.!"
                            }
                        ]
                    }
                ]
            }
        ],
        "team": "T04MCQMEXQ9",
        "user_team": "T04MCQMEXQ9",
        "source_team": "T04MCQMEXQ9",
        "user_profile": {
            "avatar_hash": "7026e9e9c4f8",
            "image_72": "https:\/\/avatars.slack-edge.com\/2023-01-29\/4699821662839_7026e9e9c4f8501ee814_72.png",
            "first_name": "김민수\/한양대",
            "real_name": "김민수\/한양대",
            "display_name": "김민수\/한양대",
            "team": "T04MCQMEXQ9",
            "name": "minsoo2333",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "attachments": [
            {
                "from_url": "https:\/\/openreview.net\/forum?id=-Aw0rrrPUF",
                "thumb_url": "https:\/\/openreview.net\/images\/openreview_logo_512.png",
                "thumb_width": 512,
                "thumb_height": 512,
                "service_icon": "https:\/\/openreview.net\/favicon.ico",
                "id": 1,
                "original_url": "https:\/\/openreview.net\/forum?id=-Aw0rrrPUF",
                "fallback": "OpenReview: GLM-130B: An Open Bilingual Pre-trained Model",
                "text": "We introduce GLM-130B, a bilingual (English and Chinese) pre-trained language model with 130 billion parameters. It is an attempt to open-source a 100B-scale model as good as GPT-3 and unveil how...",
                "title": "GLM-130B: An Open Bilingual Pre-trained Model",
                "title_link": "https:\/\/openreview.net\/forum?id=-Aw0rrrPUF",
                "service_name": "OpenReview"
            },
            {
                "from_url": "https:\/\/github.com\/THUDM\/GLM-130B",
                "image_url": "https:\/\/opengraph.githubassets.com\/0313c4bf72952eb3aaaf62c6b40bc2d70d459b69514c55d11883cf3242e29b5e\/THUDM\/GLM-130B",
                "image_width": 500,
                "image_height": 250,
                "image_bytes": 94475,
                "service_icon": "https:\/\/a.slack-edge.com\/80588\/img\/unfurl_icons\/github.png",
                "id": 2,
                "original_url": "https:\/\/github.com\/THUDM\/GLM-130B",
                "fallback": "GitHub: GitHub - THUDM\/GLM-130B: GLM-130B: An Open Bilingual Pre-Trained Model (ICLR 2023)",
                "text": "GLM-130B: An Open Bilingual Pre-Trained Model (ICLR 2023) - GitHub - THUDM\/GLM-130B: GLM-130B: An Open Bilingual Pre-Trained Model (ICLR 2023)",
                "title": "GitHub - THUDM\/GLM-130B: GLM-130B: An Open Bilingual Pre-Trained Model (ICLR 2023)",
                "title_link": "https:\/\/github.com\/THUDM\/GLM-130B",
                "service_name": "GitHub"
            }
        ],
        "thread_ts": "1675336960.897349",
        "reply_count": 15,
        "reply_users_count": 3,
        "latest_reply": "1677287401.436479",
        "reply_users": [
            "U04M2NY6U2Y",
            "U04M05UHX7U",
            "U04LL3W56F9"
        ],
        "replies": [
            {
                "user": "U04M2NY6U2Y",
                "ts": "1675337014.407389"
            },
            {
                "user": "U04M2NY6U2Y",
                "ts": "1675337042.442879"
            },
            {
                "user": "U04M2NY6U2Y",
                "ts": "1675337069.986769"
            },
            {
                "user": "U04M2NY6U2Y",
                "ts": "1675337080.286219"
            },
            {
                "user": "U04M05UHX7U",
                "ts": "1675341427.468399"
            },
            {
                "user": "U04M2NY6U2Y",
                "ts": "1675385708.005829"
            },
            {
                "user": "U04M05UHX7U",
                "ts": "1675388921.973739"
            },
            {
                "user": "U04M2NY6U2Y",
                "ts": "1675389806.285319"
            },
            {
                "user": "U04M05UHX7U",
                "ts": "1675389908.068079"
            },
            {
                "user": "U04M2NY6U2Y",
                "ts": "1675390280.712549"
            },
            {
                "user": "U04M05UHX7U",
                "ts": "1675390324.321049"
            },
            {
                "user": "U04LL3W56F9",
                "ts": "1675413310.805959"
            },
            {
                "user": "U04M05UHX7U",
                "ts": "1675419648.433509"
            },
            {
                "user": "U04LL3W56F9",
                "ts": "1675662059.345069"
            },
            {
                "user": "U04M05UHX7U",
                "ts": "1677287401.436479"
            }
        ],
        "is_locked": false,
        "subscribed": true,
        "last_read": "1677287401.436479",
        "reactions": [
            {
                "name": "+1",
                "users": [
                    "U04M05UHX7U",
                    "U04M0CJUUCT",
                    "U04LYGJ3945",
                    "U04MQ1B2BGQ",
                    "U04M06CM95H",
                    "U04MD0GTCG1",
                    "U04M0S8UMEW",
                    "U04M091M3MZ",
                    "U04LV2JHHB8",
                    "U04M31F24CU",
                    "U04M0Q6HTQA",
                    "U04MB9UUFK5",
                    "U04LTMVAQ4W",
                    "U04MR94VA68",
                    "U04M3V6RJLF",
                    "U04MQHDJ12L",
                    "U04M059CL67",
                    "U04LXKFJLFP",
                    "U04LKPBUQ23",
                    "U04MQ2J38JC",
                    "U04LKU43D7Z"
                ],
                "count": 21
            }
        ]
    },
    {
        "type": "message",
        "text": "*General Language Modeling (GLM)*\n\n• ACL 2022 에서 발표된 Bidirectional Pre-Trained GLM 을  Backbone 으로 채택하고 있습니다.\n• Input 으로 들어오는 Context 문장은 BERT 처럼 Bidirectional 하게 보고, 빈칸 채우기는 GPT 처럼 Uni-Directional 하게 Pre-Training 하는 방식 입니다. (그림 참조)\n• T5 에서 사용하는 MIP (Multi-Task Instruction Pre-Training) 도 Pre-Training 과정에서 함께 사용하고 있어, zero-shot Evaluation Application 선정에 대해 Open Review 에서 많은 논의가 오고간 것 같습니다.\n• 이러한 Objective 설정 및 Pre-Training 을 통해 Model Parameter 수가 더 적음에도 Zero-Shot LAMBADA 성능이 더 높다는 걸 확인할 수 있고, 왼쪽 Bar Plot 의 4,5번째를 비교하면 Uni-Directional 한 Attending 이 성능 향상에 꽤 큰 영향을 미치는 점을 확인할 수 있습니다.\n",
        "files": [
            {
                "id": "F04MJ83HN4E",
                "created": 1675337000,
                "timestamp": 1675337000,
                "name": "image.png",
                "title": "image.png",
                "mimetype": "image\/png",
                "filetype": "png",
                "pretty_type": "PNG",
                "user": "U04M2NY6U2Y",
                "user_team": "T04MCQMEXQ9",
                "editable": false,
                "size": 216312,
                "mode": "hosted",
                "is_external": false,
                "external_type": "",
                "is_public": true,
                "public_url_shared": false,
                "display_as_bot": false,
                "username": "",
                "url_private": "https:\/\/files.slack.com\/files-pri\/T04MCQMEXQ9-F04MJ83HN4E\/image.png?t=xoxe-4726837507825-4848681849303-4856614048758-e0b1f3d4cb371f92260edb0d9444d206",
                "url_private_download": "https:\/\/files.slack.com\/files-pri\/T04MCQMEXQ9-F04MJ83HN4E\/download\/image.png?t=xoxe-4726837507825-4848681849303-4856614048758-e0b1f3d4cb371f92260edb0d9444d206",
                "media_display_type": "unknown",
                "thumb_64": "https:\/\/files.slack.com\/files-tmb\/T04MCQMEXQ9-F04MJ83HN4E-72800fca50\/image_64.png?t=xoxe-4726837507825-4848681849303-4856614048758-e0b1f3d4cb371f92260edb0d9444d206",
                "thumb_80": "https:\/\/files.slack.com\/files-tmb\/T04MCQMEXQ9-F04MJ83HN4E-72800fca50\/image_80.png?t=xoxe-4726837507825-4848681849303-4856614048758-e0b1f3d4cb371f92260edb0d9444d206",
                "thumb_360": "https:\/\/files.slack.com\/files-tmb\/T04MCQMEXQ9-F04MJ83HN4E-72800fca50\/image_360.png?t=xoxe-4726837507825-4848681849303-4856614048758-e0b1f3d4cb371f92260edb0d9444d206",
                "thumb_360_w": 360,
                "thumb_360_h": 338,
                "thumb_480": "https:\/\/files.slack.com\/files-tmb\/T04MCQMEXQ9-F04MJ83HN4E-72800fca50\/image_480.png?t=xoxe-4726837507825-4848681849303-4856614048758-e0b1f3d4cb371f92260edb0d9444d206",
                "thumb_480_w": 480,
                "thumb_480_h": 450,
                "thumb_160": "https:\/\/files.slack.com\/files-tmb\/T04MCQMEXQ9-F04MJ83HN4E-72800fca50\/image_160.png?t=xoxe-4726837507825-4848681849303-4856614048758-e0b1f3d4cb371f92260edb0d9444d206",
                "thumb_720": "https:\/\/files.slack.com\/files-tmb\/T04MCQMEXQ9-F04MJ83HN4E-72800fca50\/image_720.png?t=xoxe-4726837507825-4848681849303-4856614048758-e0b1f3d4cb371f92260edb0d9444d206",
                "thumb_720_w": 720,
                "thumb_720_h": 675,
                "thumb_800": "https:\/\/files.slack.com\/files-tmb\/T04MCQMEXQ9-F04MJ83HN4E-72800fca50\/image_800.png?t=xoxe-4726837507825-4848681849303-4856614048758-e0b1f3d4cb371f92260edb0d9444d206",
                "thumb_800_w": 800,
                "thumb_800_h": 750,
                "thumb_960": "https:\/\/files.slack.com\/files-tmb\/T04MCQMEXQ9-F04MJ83HN4E-72800fca50\/image_960.png?t=xoxe-4726837507825-4848681849303-4856614048758-e0b1f3d4cb371f92260edb0d9444d206",
                "thumb_960_w": 960,
                "thumb_960_h": 900,
                "thumb_1024": "https:\/\/files.slack.com\/files-tmb\/T04MCQMEXQ9-F04MJ83HN4E-72800fca50\/image_1024.png?t=xoxe-4726837507825-4848681849303-4856614048758-e0b1f3d4cb371f92260edb0d9444d206",
                "thumb_1024_w": 1024,
                "thumb_1024_h": 960,
                "original_w": 1542,
                "original_h": 1446,
                "thumb_tiny": "AwAtADDSzg89KQkf3v1pkzlELAc9KRJgVTcOW9KPMB7kIm4k4HvSowZARnHvTLj\/AFLVHHKI40XGc\/40DtoTk4NIp3GhutCfeJoER3X+pb8KiXpF\/nvViZC6EA46VGISBHz09veh7DW4+4IMLVVPSL\/PepzESHGevt70hgOE5+77deaFuCehM2c0ITmhs56GhQQ3Q0CGEgOfmwM\/36CRgHj\/AL6p5Qkk7j+Qo2H+9+goAj4IIyMDvvoJ+bqOv96pNh\/vn8hRsOfvH8qAIwQM8jOc430KfmHPf+\/Uuw\/3v0oVMHk5+tAH\/9k=",
                "permalink": "https:\/\/effai.slack.com\/files\/U04M2NY6U2Y\/F04MJ83HN4E\/image.png",
                "permalink_public": "https:\/\/slack-files.com\/T04MCQMEXQ9-F04MJ83HN4E-af13691dad",
                "is_starred": false,
                "has_rich_preview": false,
                "file_access": "visible"
            }
        ],
        "upload": false,
        "user": "U04M2NY6U2Y",
        "display_as_bot": false,
        "ts": "1675337014.407389",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "e214t",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "General Language Modeling (GLM)",
                                "style": {
                                    "bold": true
                                }
                            },
                            {
                                "type": "text",
                                "text": "\n\n"
                            }
                        ]
                    },
                    {
                        "type": "rich_text_list",
                        "elements": [
                            {
                                "type": "rich_text_section",
                                "elements": [
                                    {
                                        "type": "text",
                                        "text": "ACL 2022 에서 발표된 Bidirectional Pre-Trained GLM 을  Backbone 으로 채택하고 있습니다."
                                    }
                                ]
                            },
                            {
                                "type": "rich_text_section",
                                "elements": [
                                    {
                                        "type": "text",
                                        "text": "Input 으로 들어오는 Context 문장은 BERT 처럼 Bidirectional 하게 보고, 빈칸 채우기는 GPT 처럼 Uni-Directional 하게 Pre-Training 하는 방식 입니다. (그림 참조)"
                                    }
                                ]
                            },
                            {
                                "type": "rich_text_section",
                                "elements": [
                                    {
                                        "type": "text",
                                        "text": "T5 에서 사용하는 MIP (Multi-Task Instruction Pre-Training) 도 Pre-Training 과정에서 함께 사용하고 있어, zero-shot Evaluation Application 선정에 대해 Open Review 에서 많은 논의가 오고간 것 같습니다."
                                    }
                                ]
                            },
                            {
                                "type": "rich_text_section",
                                "elements": [
                                    {
                                        "type": "text",
                                        "text": "이러한 Objective 설정 및 Pre-Training 을 통해 Model Parameter 수가 더 적음에도 Zero-Shot LAMBADA 성능이 더 높다는 걸 확인할 수 있고, 왼쪽 Bar Plot 의 4,5번째를 비교하면 Uni-Directional 한 Attending 이 성능 향상에 꽤 큰 영향을 미치는 점을 확인할 수 있습니다."
                                    }
                                ]
                            }
                        ],
                        "style": "bullet",
                        "indent": 0,
                        "border": 0
                    },
                    {
                        "type": "rich_text_section",
                        "elements": []
                    }
                ]
            }
        ],
        "client_msg_id": "92ecf6a3-4878-4ca5-ae3f-35bcf45f81e1",
        "thread_ts": "1675336960.897349",
        "parent_user_id": "U04M2NY6U2Y"
    },
    {
        "type": "message",
        "text": "*Training Instability*\n• Layer Normalization\n    ◦ 기존 LLM 에서 자주 사용되는 Pre-LN 방식 (Layer Norm 이 Sub-Layer Input 에 위치, GPT\/ViT 에서 사용) 및 Post-LN 방식 (Layer Norm 이 Sub-Layer Residual Path 뒤에 위치, BERT 에서 사용)  이 Training Instability 문제를 가지고 있어 학습에 어려움이 있다고 제시합니다. (첫번째 그림 (a)  Gradient Splike)\n    ◦ GLM-130B 은 Post-LN + DeepNorm 이 Training Instability 해결에 효과적이라고 제시합니다. (DeepNorm 은 DeepNet : Scaling Transformers to 1000 layers 라는 논문에서 제시된 방법) DeepNorm 은 Residual Path 에 Scale 을 곱해 LN 에 넣어주는 방식, Weight Initialization 을 일부는 다르게 해준다는 특징이 있습니다.\n        ▪︎ DeepNorm(x) = LayerNorm (ax + Network(x)), where a = (2N)^(1\/2)\n        ▪︎ FFN, Attention Value \/ Output Projection 에 Xavier Initialization 방식 사용\n• Embedding layer Gradient Shrink (EGS)\n    ◦ 많은 Pre-Training 시행착오를 통해, Training Collapse 의 아주 좋은 Indicator 가 Gradient Norm 이라는 점을 발견했다 하며, Gradient Spike 를 줄이면 Pre-Training Collapse 를 방지할 수 있다고 합니다.\n    ◦ Gradient Spike 는 주로 Embedding Layer 의 Gradient 에서 자주 발생된다는 점을 발견했고 (두번째 그림 (a)), 이 Gradient 에 Shrinking Scale (0.1) 을 걸어주면 학습 안정화에 도움이 된다고 합니다. (두번째 그림 (b) (Scale 가해주는건 LSQ 의 Gradient Scaling 방식과 유사)\n",
        "files": [
            {
                "id": "F04MAA1PKE3",
                "created": 1675337032,
                "timestamp": 1675337032,
                "name": "image.png",
                "title": "image.png",
                "mimetype": "image\/png",
                "filetype": "png",
                "pretty_type": "PNG",
                "user": "U04M2NY6U2Y",
                "user_team": "T04MCQMEXQ9",
                "editable": false,
                "size": 508713,
                "mode": "hosted",
                "is_external": false,
                "external_type": "",
                "is_public": true,
                "public_url_shared": false,
                "display_as_bot": false,
                "username": "",
                "url_private": "https:\/\/files.slack.com\/files-pri\/T04MCQMEXQ9-F04MAA1PKE3\/image.png?t=xoxe-4726837507825-4848681849303-4856614048758-e0b1f3d4cb371f92260edb0d9444d206",
                "url_private_download": "https:\/\/files.slack.com\/files-pri\/T04MCQMEXQ9-F04MAA1PKE3\/download\/image.png?t=xoxe-4726837507825-4848681849303-4856614048758-e0b1f3d4cb371f92260edb0d9444d206",
                "media_display_type": "unknown",
                "thumb_64": "https:\/\/files.slack.com\/files-tmb\/T04MCQMEXQ9-F04MAA1PKE3-b3a7fa7ffa\/image_64.png?t=xoxe-4726837507825-4848681849303-4856614048758-e0b1f3d4cb371f92260edb0d9444d206",
                "thumb_80": "https:\/\/files.slack.com\/files-tmb\/T04MCQMEXQ9-F04MAA1PKE3-b3a7fa7ffa\/image_80.png?t=xoxe-4726837507825-4848681849303-4856614048758-e0b1f3d4cb371f92260edb0d9444d206",
                "thumb_360": "https:\/\/files.slack.com\/files-tmb\/T04MCQMEXQ9-F04MAA1PKE3-b3a7fa7ffa\/image_360.png?t=xoxe-4726837507825-4848681849303-4856614048758-e0b1f3d4cb371f92260edb0d9444d206",
                "thumb_360_w": 360,
                "thumb_360_h": 114,
                "thumb_480": "https:\/\/files.slack.com\/files-tmb\/T04MCQMEXQ9-F04MAA1PKE3-b3a7fa7ffa\/image_480.png?t=xoxe-4726837507825-4848681849303-4856614048758-e0b1f3d4cb371f92260edb0d9444d206",
                "thumb_480_w": 480,
                "thumb_480_h": 152,
                "thumb_160": "https:\/\/files.slack.com\/files-tmb\/T04MCQMEXQ9-F04MAA1PKE3-b3a7fa7ffa\/image_160.png?t=xoxe-4726837507825-4848681849303-4856614048758-e0b1f3d4cb371f92260edb0d9444d206",
                "thumb_720": "https:\/\/files.slack.com\/files-tmb\/T04MCQMEXQ9-F04MAA1PKE3-b3a7fa7ffa\/image_720.png?t=xoxe-4726837507825-4848681849303-4856614048758-e0b1f3d4cb371f92260edb0d9444d206",
                "thumb_720_w": 720,
                "thumb_720_h": 228,
                "thumb_800": "https:\/\/files.slack.com\/files-tmb\/T04MCQMEXQ9-F04MAA1PKE3-b3a7fa7ffa\/image_800.png?t=xoxe-4726837507825-4848681849303-4856614048758-e0b1f3d4cb371f92260edb0d9444d206",
                "thumb_800_w": 800,
                "thumb_800_h": 254,
                "thumb_960": "https:\/\/files.slack.com\/files-tmb\/T04MCQMEXQ9-F04MAA1PKE3-b3a7fa7ffa\/image_960.png?t=xoxe-4726837507825-4848681849303-4856614048758-e0b1f3d4cb371f92260edb0d9444d206",
                "thumb_960_w": 960,
                "thumb_960_h": 304,
                "thumb_1024": "https:\/\/files.slack.com\/files-tmb\/T04MCQMEXQ9-F04MAA1PKE3-b3a7fa7ffa\/image_1024.png?t=xoxe-4726837507825-4848681849303-4856614048758-e0b1f3d4cb371f92260edb0d9444d206",
                "thumb_1024_w": 1024,
                "thumb_1024_h": 325,
                "original_w": 2000,
                "original_h": 634,
                "thumb_tiny": "AwAPADC7MxAQg96SViyKVP5GnOEcAN2pojjB4ZqBodE2UB9qcx2sCScEYpq7Q2d5PtinFwD1oELg\/wB6lwQetM3AdOKXfg80Af\/Z",
                "permalink": "https:\/\/effai.slack.com\/files\/U04M2NY6U2Y\/F04MAA1PKE3\/image.png",
                "permalink_public": "https:\/\/slack-files.com\/T04MCQMEXQ9-F04MAA1PKE3-01c71a9d11",
                "is_starred": false,
                "has_rich_preview": false,
                "file_access": "visible"
            },
            {
                "id": "F04N3F83D6D",
                "created": 1675337037,
                "timestamp": 1675337037,
                "name": "image.png",
                "title": "image.png",
                "mimetype": "image\/png",
                "filetype": "png",
                "pretty_type": "PNG",
                "user": "U04M2NY6U2Y",
                "user_team": "T04MCQMEXQ9",
                "editable": false,
                "size": 403351,
                "mode": "hosted",
                "is_external": false,
                "external_type": "",
                "is_public": true,
                "public_url_shared": false,
                "display_as_bot": false,
                "username": "",
                "url_private": "https:\/\/files.slack.com\/files-pri\/T04MCQMEXQ9-F04N3F83D6D\/image.png?t=xoxe-4726837507825-4848681849303-4856614048758-e0b1f3d4cb371f92260edb0d9444d206",
                "url_private_download": "https:\/\/files.slack.com\/files-pri\/T04MCQMEXQ9-F04N3F83D6D\/download\/image.png?t=xoxe-4726837507825-4848681849303-4856614048758-e0b1f3d4cb371f92260edb0d9444d206",
                "media_display_type": "unknown",
                "thumb_64": "https:\/\/files.slack.com\/files-tmb\/T04MCQMEXQ9-F04N3F83D6D-3bfe6fcd16\/image_64.png?t=xoxe-4726837507825-4848681849303-4856614048758-e0b1f3d4cb371f92260edb0d9444d206",
                "thumb_80": "https:\/\/files.slack.com\/files-tmb\/T04MCQMEXQ9-F04N3F83D6D-3bfe6fcd16\/image_80.png?t=xoxe-4726837507825-4848681849303-4856614048758-e0b1f3d4cb371f92260edb0d9444d206",
                "thumb_360": "https:\/\/files.slack.com\/files-tmb\/T04MCQMEXQ9-F04N3F83D6D-3bfe6fcd16\/image_360.png?t=xoxe-4726837507825-4848681849303-4856614048758-e0b1f3d4cb371f92260edb0d9444d206",
                "thumb_360_w": 360,
                "thumb_360_h": 139,
                "thumb_480": "https:\/\/files.slack.com\/files-tmb\/T04MCQMEXQ9-F04N3F83D6D-3bfe6fcd16\/image_480.png?t=xoxe-4726837507825-4848681849303-4856614048758-e0b1f3d4cb371f92260edb0d9444d206",
                "thumb_480_w": 480,
                "thumb_480_h": 185,
                "thumb_160": "https:\/\/files.slack.com\/files-tmb\/T04MCQMEXQ9-F04N3F83D6D-3bfe6fcd16\/image_160.png?t=xoxe-4726837507825-4848681849303-4856614048758-e0b1f3d4cb371f92260edb0d9444d206",
                "thumb_720": "https:\/\/files.slack.com\/files-tmb\/T04MCQMEXQ9-F04N3F83D6D-3bfe6fcd16\/image_720.png?t=xoxe-4726837507825-4848681849303-4856614048758-e0b1f3d4cb371f92260edb0d9444d206",
                "thumb_720_w": 720,
                "thumb_720_h": 277,
                "thumb_800": "https:\/\/files.slack.com\/files-tmb\/T04MCQMEXQ9-F04N3F83D6D-3bfe6fcd16\/image_800.png?t=xoxe-4726837507825-4848681849303-4856614048758-e0b1f3d4cb371f92260edb0d9444d206",
                "thumb_800_w": 800,
                "thumb_800_h": 308,
                "thumb_960": "https:\/\/files.slack.com\/files-tmb\/T04MCQMEXQ9-F04N3F83D6D-3bfe6fcd16\/image_960.png?t=xoxe-4726837507825-4848681849303-4856614048758-e0b1f3d4cb371f92260edb0d9444d206",
                "thumb_960_w": 960,
                "thumb_960_h": 370,
                "thumb_1024": "https:\/\/files.slack.com\/files-tmb\/T04MCQMEXQ9-F04N3F83D6D-3bfe6fcd16\/image_1024.png?t=xoxe-4726837507825-4848681849303-4856614048758-e0b1f3d4cb371f92260edb0d9444d206",
                "thumb_1024_w": 1024,
                "thumb_1024_h": 394,
                "original_w": 2000,
                "original_h": 770,
                "thumb_tiny": "AwASADDRJ79OaDu7ZoYDBB70xdyjAYEe9AyQZ7igHmmhj3x+FLgDr3oEKRk9aTo2N1HfhqAe+c\/hQApAPUCjaPQUtFABgegoIB6iiigBNo9B+VAAB4ApaKAP\/9k=",
                "permalink": "https:\/\/effai.slack.com\/files\/U04M2NY6U2Y\/F04N3F83D6D\/image.png",
                "permalink_public": "https:\/\/slack-files.com\/T04MCQMEXQ9-F04N3F83D6D-a7d1daa838",
                "is_starred": false,
                "has_rich_preview": false,
                "file_access": "visible"
            }
        ],
        "upload": false,
        "user": "U04M2NY6U2Y",
        "ts": "1675337042.442879",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "3GWl8",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "Training Instability",
                                "style": {
                                    "bold": true
                                }
                            },
                            {
                                "type": "text",
                                "text": "\n"
                            }
                        ]
                    },
                    {
                        "type": "rich_text_list",
                        "elements": [
                            {
                                "type": "rich_text_section",
                                "elements": [
                                    {
                                        "type": "text",
                                        "text": "Layer Normalization"
                                    }
                                ]
                            }
                        ],
                        "style": "bullet",
                        "indent": 0,
                        "border": 0
                    },
                    {
                        "type": "rich_text_list",
                        "elements": [
                            {
                                "type": "rich_text_section",
                                "elements": [
                                    {
                                        "type": "text",
                                        "text": "기존 LLM 에서 자주 사용되는 Pre-LN 방식 (Layer Norm 이 Sub-Layer Input 에 위치, GPT\/ViT 에서 사용) 및 Post-LN 방식 (Layer Norm 이 Sub-Layer Residual Path 뒤에 위치, BERT 에서 사용)  이 Training Instability 문제를 가지고 있어 학습에 어려움이 있다고 제시합니다. (첫번째 그림 (a)  Gradient Splike)"
                                    }
                                ]
                            },
                            {
                                "type": "rich_text_section",
                                "elements": [
                                    {
                                        "type": "text",
                                        "text": "GLM-130B 은 Post-LN + DeepNorm 이 Training Instability 해결에 효과적이라고 제시합니다. (DeepNorm 은 DeepNet : Scaling Transformers to 1000 layers 라는 논문에서 제시된 방법) DeepNorm 은 Residual Path 에 Scale 을 곱해 LN 에 넣어주는 방식, Weight Initialization 을 일부는 다르게 해준다는 특징이 있습니다."
                                    }
                                ]
                            }
                        ],
                        "style": "bullet",
                        "indent": 1,
                        "border": 0
                    },
                    {
                        "type": "rich_text_list",
                        "elements": [
                            {
                                "type": "rich_text_section",
                                "elements": [
                                    {
                                        "type": "text",
                                        "text": "DeepNorm(x) = LayerNorm (ax + Network(x)), where a = (2N)^(1\/2)"
                                    }
                                ]
                            },
                            {
                                "type": "rich_text_section",
                                "elements": [
                                    {
                                        "type": "text",
                                        "text": "FFN, Attention Value \/ Output Projection 에 Xavier Initialization 방식 사용"
                                    }
                                ]
                            }
                        ],
                        "style": "bullet",
                        "indent": 2,
                        "border": 0
                    },
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "\n"
                            }
                        ]
                    },
                    {
                        "type": "rich_text_list",
                        "elements": [
                            {
                                "type": "rich_text_section",
                                "elements": [
                                    {
                                        "type": "text",
                                        "text": "Embedding layer Gradient Shrink (EGS)"
                                    }
                                ]
                            }
                        ],
                        "style": "bullet",
                        "indent": 0,
                        "border": 0
                    },
                    {
                        "type": "rich_text_list",
                        "elements": [
                            {
                                "type": "rich_text_section",
                                "elements": [
                                    {
                                        "type": "text",
                                        "text": "많은 Pre-Training 시행착오를 통해, Training Collapse 의 아주 좋은 Indicator 가 Gradient Norm 이라는 점을 발견했다 하며, Gradient Spike 를 줄이면 Pre-Training Collapse 를 방지할 수 있다고 합니다."
                                    }
                                ]
                            },
                            {
                                "type": "rich_text_section",
                                "elements": [
                                    {
                                        "type": "text",
                                        "text": "Gradient Spike 는 주로 Embedding Layer 의 Gradient 에서 자주 발생된다는 점을 발견했고 (두번째 그림 (a)), 이 Gradient 에 Shrinking Scale (0.1) 을 걸어주면 학습 안정화에 도움이 된다고 합니다. (두번째 그림 (b) (Scale 가해주는건 LSQ 의 Gradient Scaling 방식과 유사)"
                                    }
                                ]
                            }
                        ],
                        "style": "bullet",
                        "indent": 1,
                        "border": 0
                    },
                    {
                        "type": "rich_text_section",
                        "elements": []
                    }
                ]
            }
        ],
        "client_msg_id": "ceab3681-587c-41c7-9429-ac759d0389ec",
        "thread_ts": "1675336960.897349",
        "parent_user_id": "U04M2NY6U2Y"
    },
    {
        "type": "message",
        "text": "*INT4 Quantization for RTX3090s\/2080s*\n\n• LLM.int8에서 보였듯이, LLM 의 Activation 에서는 0.1% 의 적은 비율이지만, 성능에 유의미한 영향을 미치는 Extreme Outlier 를 가지고 있습니다. (OPT-175B\/BLOOM-176B)\n• GLM-130B 에서는 이러한 Outlier 가 전체의 30% 에서 발생된다고 하며, Acitvation 은 따라서 FP16 으로 두고, Weight Quantization 에 집중한다고 합니다.\n• 흥미롭게도, GLM-130B 의 Weight Distribution 은 다른 LLM 에 비해 훨씬 더 Narrow 한 Distribution 을 가지고 있어, 성능 하락 거의 없이 PTQ 로 INT4 Weight Quantization 까지 적용할 수 있다고 합니다. (LAMBADA -0.74%)\n*Discussion*\n• Rebuttal 에서 한 리뷰어가 Weight Distribution 이 좁아지는 이유에 대해 물었고, LayerNorm 의 옵션이 영향이 있지 않을까라고 제안해보았습니다.\n    ◦ 아쉽게 130B 에 대한 Ablation Study 는 볼 수 없었지만, 100M 급 GLM 모델에서의 LN 옵션에 따른 Weight 분포를 보면, LN 이 Distribution 모양에 큰 영향을 미치는 것 처럼 보이진 않는다고 답변하고 있습니다.\n    ◦ 저자는 GLM 의 Pre-Training 의 Objective 가 이러한 결과를 가져온 것 같다고 추측 (Future Work) 하고 있는데, 이 글을 읽으시는 여러분들의 생각은 어떠실지 궁금합니다.\n    ◦ 저는 개인적으로 Activation 에서 Outlier 가 30% 까지 늘어난 것과 Weight Distribution 이 좁아진 것이 연관이 있을 것 같다는 생각이 듭니다만, 어떻게 이런 결과가 나타난건지 궁금하더라구요.. (마치 Song Han 의 Smooth Quant 방법의 결과물이 Pre-Training 을 거치면서 나타난 것 같다고나 할까요..)\n",
        "files": [
            {
                "id": "F04MAA3J2MV",
                "created": 1675337058,
                "timestamp": 1675337058,
                "name": "image.png",
                "title": "image.png",
                "mimetype": "image\/png",
                "filetype": "png",
                "pretty_type": "PNG",
                "user": "U04M2NY6U2Y",
                "user_team": "T04MCQMEXQ9",
                "editable": false,
                "size": 167279,
                "mode": "hosted",
                "is_external": false,
                "external_type": "",
                "is_public": true,
                "public_url_shared": false,
                "display_as_bot": false,
                "username": "",
                "url_private": "https:\/\/files.slack.com\/files-pri\/T04MCQMEXQ9-F04MAA3J2MV\/image.png?t=xoxe-4726837507825-4848681849303-4856614048758-e0b1f3d4cb371f92260edb0d9444d206",
                "url_private_download": "https:\/\/files.slack.com\/files-pri\/T04MCQMEXQ9-F04MAA3J2MV\/download\/image.png?t=xoxe-4726837507825-4848681849303-4856614048758-e0b1f3d4cb371f92260edb0d9444d206",
                "media_display_type": "unknown",
                "thumb_64": "https:\/\/files.slack.com\/files-tmb\/T04MCQMEXQ9-F04MAA3J2MV-e32a28372e\/image_64.png?t=xoxe-4726837507825-4848681849303-4856614048758-e0b1f3d4cb371f92260edb0d9444d206",
                "thumb_80": "https:\/\/files.slack.com\/files-tmb\/T04MCQMEXQ9-F04MAA3J2MV-e32a28372e\/image_80.png?t=xoxe-4726837507825-4848681849303-4856614048758-e0b1f3d4cb371f92260edb0d9444d206",
                "thumb_360": "https:\/\/files.slack.com\/files-tmb\/T04MCQMEXQ9-F04MAA3J2MV-e32a28372e\/image_360.png?t=xoxe-4726837507825-4848681849303-4856614048758-e0b1f3d4cb371f92260edb0d9444d206",
                "thumb_360_w": 360,
                "thumb_360_h": 71,
                "thumb_480": "https:\/\/files.slack.com\/files-tmb\/T04MCQMEXQ9-F04MAA3J2MV-e32a28372e\/image_480.png?t=xoxe-4726837507825-4848681849303-4856614048758-e0b1f3d4cb371f92260edb0d9444d206",
                "thumb_480_w": 480,
                "thumb_480_h": 95,
                "thumb_160": "https:\/\/files.slack.com\/files-tmb\/T04MCQMEXQ9-F04MAA3J2MV-e32a28372e\/image_160.png?t=xoxe-4726837507825-4848681849303-4856614048758-e0b1f3d4cb371f92260edb0d9444d206",
                "thumb_720": "https:\/\/files.slack.com\/files-tmb\/T04MCQMEXQ9-F04MAA3J2MV-e32a28372e\/image_720.png?t=xoxe-4726837507825-4848681849303-4856614048758-e0b1f3d4cb371f92260edb0d9444d206",
                "thumb_720_w": 720,
                "thumb_720_h": 142,
                "thumb_800": "https:\/\/files.slack.com\/files-tmb\/T04MCQMEXQ9-F04MAA3J2MV-e32a28372e\/image_800.png?t=xoxe-4726837507825-4848681849303-4856614048758-e0b1f3d4cb371f92260edb0d9444d206",
                "thumb_800_w": 800,
                "thumb_800_h": 158,
                "thumb_960": "https:\/\/files.slack.com\/files-tmb\/T04MCQMEXQ9-F04MAA3J2MV-e32a28372e\/image_960.png?t=xoxe-4726837507825-4848681849303-4856614048758-e0b1f3d4cb371f92260edb0d9444d206",
                "thumb_960_w": 960,
                "thumb_960_h": 190,
                "thumb_1024": "https:\/\/files.slack.com\/files-tmb\/T04MCQMEXQ9-F04MAA3J2MV-e32a28372e\/image_1024.png?t=xoxe-4726837507825-4848681849303-4856614048758-e0b1f3d4cb371f92260edb0d9444d206",
                "thumb_1024_w": 1024,
                "thumb_1024_h": 202,
                "original_w": 2000,
                "original_h": 395,
                "thumb_tiny": "AwAJADC5cyMqAICWJ7CpVYMobGM9jTqKAIY5N0zgg47HFLO5RBtBJyOgqWigBMg4NNyC5HoKfRQB\/9k=",
                "permalink": "https:\/\/effai.slack.com\/files\/U04M2NY6U2Y\/F04MAA3J2MV\/image.png",
                "permalink_public": "https:\/\/slack-files.com\/T04MCQMEXQ9-F04MAA3J2MV-cd36fa4d43",
                "is_starred": false,
                "has_rich_preview": false,
                "file_access": "visible"
            }
        ],
        "upload": false,
        "user": "U04M2NY6U2Y",
        "display_as_bot": false,
        "ts": "1675337069.986769",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "7fEji",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "INT4 Quantization for RTX3090s\/2080s",
                                "style": {
                                    "bold": true
                                }
                            },
                            {
                                "type": "text",
                                "text": "\n\n"
                            }
                        ]
                    },
                    {
                        "type": "rich_text_list",
                        "elements": [
                            {
                                "type": "rich_text_section",
                                "elements": [
                                    {
                                        "type": "text",
                                        "text": "LLM.int8에서 보였듯이, LLM 의 Activation 에서는 0.1% 의 적은 비율이지만, 성능에 유의미한 영향을 미치는 Extreme Outlier 를 가지고 있습니다. (OPT-175B\/BLOOM-176B)"
                                    }
                                ]
                            },
                            {
                                "type": "rich_text_section",
                                "elements": [
                                    {
                                        "type": "text",
                                        "text": "GLM-130B 에서는 이러한 Outlier 가 전체의 30% 에서 발생된다고 하며, Acitvation 은 따라서 FP16 으로 두고, Weight Quantization 에 집중한다고 합니다."
                                    }
                                ]
                            },
                            {
                                "type": "rich_text_section",
                                "elements": [
                                    {
                                        "type": "text",
                                        "text": "흥미롭게도, GLM-130B 의 Weight Distribution 은 다른 LLM 에 비해 훨씬 더 Narrow 한 Distribution 을 가지고 있어, 성능 하락 거의 없이 PTQ 로 INT4 Weight Quantization 까지 적용할 수 있다고 합니다. (LAMBADA -0.74%)"
                                    }
                                ]
                            }
                        ],
                        "style": "bullet",
                        "indent": 0,
                        "border": 0
                    },
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "\n"
                            },
                            {
                                "type": "text",
                                "text": "Discussion",
                                "style": {
                                    "bold": true
                                }
                            },
                            {
                                "type": "text",
                                "text": "\n"
                            }
                        ]
                    },
                    {
                        "type": "rich_text_list",
                        "elements": [
                            {
                                "type": "rich_text_section",
                                "elements": [
                                    {
                                        "type": "text",
                                        "text": "Rebuttal 에서 한 리뷰어가 Weight Distribution 이 좁아지는 이유에 대해 물었고, LayerNorm 의 옵션이 영향이 있지 않을까라고 제안해보았습니다."
                                    }
                                ]
                            }
                        ],
                        "style": "bullet",
                        "indent": 0,
                        "border": 0
                    },
                    {
                        "type": "rich_text_list",
                        "elements": [
                            {
                                "type": "rich_text_section",
                                "elements": [
                                    {
                                        "type": "text",
                                        "text": "아쉽게 130B 에 대한 Ablation Study 는 볼 수 없었지만, 100M 급 GLM 모델에서의 LN 옵션에 따른 Weight 분포를 보면, LN 이 Distribution 모양에 큰 영향을 미치는 것 처럼 보이진 않는다고 답변하고 있습니다."
                                    }
                                ]
                            },
                            {
                                "type": "rich_text_section",
                                "elements": [
                                    {
                                        "type": "text",
                                        "text": "저자는 GLM 의 Pre-Training 의 Objective 가 이러한 결과를 가져온 것 같다고 추측 (Future Work) 하고 있는데, 이 글을 읽으시는 여러분들의 생각은 어떠실지 궁금합니다."
                                    }
                                ]
                            },
                            {
                                "type": "rich_text_section",
                                "elements": [
                                    {
                                        "type": "text",
                                        "text": "저는 개인적으로 Activation 에서 Outlier 가 30% 까지 늘어난 것과 Weight Distribution 이 좁아진 것이 연관이 있을 것 같다는 생각이 듭니다만, 어떻게 이런 결과가 나타난건지 궁금하더라구요.. (마치 Song Han 의 Smooth Quant 방법의 결과물이 Pre-Training 을 거치면서 나타난 것 같다고나 할까요..)"
                                    }
                                ]
                            }
                        ],
                        "style": "bullet",
                        "indent": 1,
                        "border": 0
                    },
                    {
                        "type": "rich_text_section",
                        "elements": []
                    }
                ]
            }
        ],
        "client_msg_id": "e4b2aa5a-1e8f-4577-b5ce-ed98755a8727",
        "thread_ts": "1675336960.897349",
        "parent_user_id": "U04M2NY6U2Y"
    },
    {
        "client_msg_id": "743f6c7a-1511-4131-9c36-44819eafc6a5",
        "type": "message",
        "text": "많이 부족하지만 읽어주셔서 감사합니다. LLM 경량화가 점점 Scale 이 커지면서 경량화는 더 어려워지고 있는 것 같은데, INT4 Weight Quantization 이 Pre-Training Objective 의 변화로 (확실하진 않지만) 가능해지는 부분이 전 특히 흥미로웠던 것 같습니다.\n\n더 궁금한 점 있으시거나 논의하고 싶은게 있다면 편하게 답글 달아주시면 감사드리겠습니다. :blush:",
        "user": "U04M2NY6U2Y",
        "ts": "1675337080.286219",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "WqVdv",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "많이 부족하지만 읽어주셔서 감사합니다. LLM 경량화가 점점 Scale 이 커지면서 경량화는 더 어려워지고 있는 것 같은데, INT4 Weight Quantization 이 Pre-Training Objective 의 변화로 (확실하진 않지만) 가능해지는 부분이 전 특히 흥미로웠던 것 같습니다.\n\n더 궁금한 점 있으시거나 논의하고 싶은게 있다면 편하게 답글 달아주시면 감사드리겠습니다. "
                            },
                            {
                                "type": "emoji",
                                "name": "blush",
                                "unicode": "1f60a"
                            }
                        ]
                    }
                ]
            }
        ],
        "team": "T04MCQMEXQ9",
        "user_team": "T04MCQMEXQ9",
        "source_team": "T04MCQMEXQ9",
        "user_profile": {
            "avatar_hash": "7026e9e9c4f8",
            "image_72": "https:\/\/avatars.slack-edge.com\/2023-01-29\/4699821662839_7026e9e9c4f8501ee814_72.png",
            "first_name": "김민수\/한양대",
            "real_name": "김민수\/한양대",
            "display_name": "김민수\/한양대",
            "team": "T04MCQMEXQ9",
            "name": "minsoo2333",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "thread_ts": "1675336960.897349",
        "parent_user_id": "U04M2NY6U2Y"
    },
    {
        "client_msg_id": "1aa6c728-7275-4949-8d95-10d73a4228e2",
        "type": "message",
        "text": "Activation 압축없이 weight 압축으로 가서 모델 성능은 최대한 건들이지 않고 LLM의  memory-bound 문제를 푸는 좋은 예라고 생각을 합니다만, 실제로 온라인으로 full-precision으로 풀어서 연산하는 Overhead가 정말 없는지에 대해서는 궁금하긴합니다. ㅎㅎ..",
        "user": "U04M05UHX7U",
        "ts": "1675341427.468399",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "FWG",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "Activation 압축없이 weight 압축으로 가서 모델 성능은 최대한 건들이지 않고 LLM의  memory-bound 문제를 푸는 좋은 예라고 생각을 합니다만, 실제로 온라인으로 full-precision으로 풀어서 연산하는 Overhead가 정말 없는지에 대해서는 궁금하긴합니다. ㅎㅎ.."
                            }
                        ]
                    }
                ]
            }
        ],
        "team": "T04MCQMEXQ9",
        "user_team": "T04MCQMEXQ9",
        "source_team": "T04MCQMEXQ9",
        "user_profile": {
            "avatar_hash": "8160001f5214",
            "image_72": "https:\/\/avatars.slack-edge.com\/2023-01-29\/4699744492023_8160001f52148d13414d_72.jpg",
            "first_name": "권세중\/네이버클라우드",
            "real_name": "권세중\/네이버클라우드",
            "display_name": "권세중\/네이버클라우드",
            "team": "T04MCQMEXQ9",
            "name": "sejung.kwon",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "thread_ts": "1675336960.897349",
        "parent_user_id": "U04M2NY6U2Y"
    },
    {
        "client_msg_id": "A99A2557-708B-4468-AC8A-5221317D019D",
        "type": "message",
        "text": "full-precision 으로 풀어서 연산한다는건 구체적으로 어떤식으로 연산하는 방식일까요? 4bit weight + FP16 Activation 연산을 위해 따라오는 overhead 를 의미하는 것이라 이해하면될까요?",
        "user": "U04M2NY6U2Y",
        "ts": "1675385708.005829",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "AlS43",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "full-precision "
                            },
                            {
                                "type": "text",
                                "text": "으로"
                            },
                            {
                                "type": "text",
                                "text": " "
                            },
                            {
                                "type": "text",
                                "text": "풀어서"
                            },
                            {
                                "type": "text",
                                "text": " "
                            },
                            {
                                "type": "text",
                                "text": "연산한다는건"
                            },
                            {
                                "type": "text",
                                "text": " "
                            },
                            {
                                "type": "text",
                                "text": "구체적으로"
                            },
                            {
                                "type": "text",
                                "text": " "
                            },
                            {
                                "type": "text",
                                "text": "어떤식으로"
                            },
                            {
                                "type": "text",
                                "text": " "
                            },
                            {
                                "type": "text",
                                "text": "연산하는"
                            },
                            {
                                "type": "text",
                                "text": " "
                            },
                            {
                                "type": "text",
                                "text": "방식일까요"
                            },
                            {
                                "type": "text",
                                "text": "? 4bit weight + FP16 Activation "
                            },
                            {
                                "type": "text",
                                "text": "연산을"
                            },
                            {
                                "type": "text",
                                "text": " "
                            },
                            {
                                "type": "text",
                                "text": "위해"
                            },
                            {
                                "type": "text",
                                "text": " "
                            },
                            {
                                "type": "text",
                                "text": "따라오는"
                            },
                            {
                                "type": "text",
                                "text": " overhead "
                            },
                            {
                                "type": "text",
                                "text": "를"
                            },
                            {
                                "type": "text",
                                "text": " "
                            },
                            {
                                "type": "text",
                                "text": "의미하는"
                            },
                            {
                                "type": "text",
                                "text": " "
                            },
                            {
                                "type": "text",
                                "text": "것이라"
                            },
                            {
                                "type": "text",
                                "text": " "
                            },
                            {
                                "type": "text",
                                "text": "이해하면될까요"
                            },
                            {
                                "type": "text",
                                "text": "?"
                            }
                        ]
                    }
                ]
            }
        ],
        "team": "T04MCQMEXQ9",
        "user_team": "T04MCQMEXQ9",
        "source_team": "T04MCQMEXQ9",
        "user_profile": {
            "avatar_hash": "7026e9e9c4f8",
            "image_72": "https:\/\/avatars.slack-edge.com\/2023-01-29\/4699821662839_7026e9e9c4f8501ee814_72.png",
            "first_name": "김민수\/한양대",
            "real_name": "김민수\/한양대",
            "display_name": "김민수\/한양대",
            "team": "T04MCQMEXQ9",
            "name": "minsoo2333",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "thread_ts": "1675336960.897349",
        "parent_user_id": "U04M2NY6U2Y"
    },
    {
        "client_msg_id": "15019149-5850-45a8-85c7-f775c56c7984",
        "type": "message",
        "text": "<@U04M2NY6U2Y> 압축된 weight INT4를 불러 올려서 FP16으로 풀어서 FP16 matmul을 한다는거에요. FP matmul 돌리는거랑 다 똑같은건데 weight 불러오는 속도만 빠르게 하는거죠.",
        "user": "U04M05UHX7U",
        "ts": "1675388921.973739",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "lAW33",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "user",
                                "user_id": "U04M2NY6U2Y"
                            },
                            {
                                "type": "text",
                                "text": " 압축된 weight INT4를 불러 올려서 FP16으로 풀어서 FP16 matmul을 한다는거에요. FP matmul 돌리는거랑 다 똑같은건데 weight 불러오는 속도만 빠르게 하는거죠."
                            }
                        ]
                    }
                ]
            }
        ],
        "team": "T04MCQMEXQ9",
        "user_team": "T04MCQMEXQ9",
        "source_team": "T04MCQMEXQ9",
        "user_profile": {
            "avatar_hash": "8160001f5214",
            "image_72": "https:\/\/avatars.slack-edge.com\/2023-01-29\/4699744492023_8160001f52148d13414d_72.jpg",
            "first_name": "권세중\/네이버클라우드",
            "real_name": "권세중\/네이버클라우드",
            "display_name": "권세중\/네이버클라우드",
            "team": "T04MCQMEXQ9",
            "name": "sejung.kwon",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "thread_ts": "1675336960.897349",
        "parent_user_id": "U04M2NY6U2Y",
        "reactions": [
            {
                "name": "bow",
                "users": [
                    "U04M2NY6U2Y"
                ],
                "count": 1
            }
        ]
    },
    {
        "type": "message",
        "text": "Appendix B.5 에 Faster Transformer Implementatio 을 통해 구현한 Inference Acceleration 결과가 있는데, Pytorch Implementation 에 비해 6-7배 빠르다는 결과도 보여주고 있더라구요. Optimization Target 을 보니 어찌보면 말씀하신 INT4 Weight 의 Dequantize 부분의 Overhead 를 보강하기 위한 최적화라고도 볼 수 있겠다는 생각이 듭니다..\n\n한 가지 궁금한 점이 있는데 Faster Trasnsformer 도 실제로 최적화를 위해 직접 사용해보시는 편인가요?",
        "files": [
            {
                "id": "F04MY7KSJ68",
                "created": 1675389471,
                "timestamp": 1675389471,
                "name": "스크린샷, 2023-02-03 오전 10.57.25.png",
                "title": "스크린샷, 2023-02-03 오전 10.57.25",
                "mimetype": "image\/png",
                "filetype": "png",
                "pretty_type": "PNG",
                "user": "U04M2NY6U2Y",
                "user_team": "T04MCQMEXQ9",
                "editable": false,
                "size": 114075,
                "mode": "hosted",
                "is_external": false,
                "external_type": "",
                "is_public": true,
                "public_url_shared": false,
                "display_as_bot": false,
                "username": "",
                "url_private": "https:\/\/files.slack.com\/files-pri\/T04MCQMEXQ9-F04MY7KSJ68\/________________________________2023-02-03_________________10.57.25.png?t=xoxe-4726837507825-4848681849303-4856614048758-e0b1f3d4cb371f92260edb0d9444d206",
                "url_private_download": "https:\/\/files.slack.com\/files-pri\/T04MCQMEXQ9-F04MY7KSJ68\/download\/________________________________2023-02-03_________________10.57.25.png?t=xoxe-4726837507825-4848681849303-4856614048758-e0b1f3d4cb371f92260edb0d9444d206",
                "media_display_type": "unknown",
                "thumb_64": "https:\/\/files.slack.com\/files-tmb\/T04MCQMEXQ9-F04MY7KSJ68-22e2022b4a\/________________________________2023-02-03_________________10.57.25_64.png?t=xoxe-4726837507825-4848681849303-4856614048758-e0b1f3d4cb371f92260edb0d9444d206",
                "thumb_80": "https:\/\/files.slack.com\/files-tmb\/T04MCQMEXQ9-F04MY7KSJ68-22e2022b4a\/________________________________2023-02-03_________________10.57.25_80.png?t=xoxe-4726837507825-4848681849303-4856614048758-e0b1f3d4cb371f92260edb0d9444d206",
                "thumb_360": "https:\/\/files.slack.com\/files-tmb\/T04MCQMEXQ9-F04MY7KSJ68-22e2022b4a\/________________________________2023-02-03_________________10.57.25_360.png?t=xoxe-4726837507825-4848681849303-4856614048758-e0b1f3d4cb371f92260edb0d9444d206",
                "thumb_360_w": 360,
                "thumb_360_h": 73,
                "thumb_480": "https:\/\/files.slack.com\/files-tmb\/T04MCQMEXQ9-F04MY7KSJ68-22e2022b4a\/________________________________2023-02-03_________________10.57.25_480.png?t=xoxe-4726837507825-4848681849303-4856614048758-e0b1f3d4cb371f92260edb0d9444d206",
                "thumb_480_w": 480,
                "thumb_480_h": 97,
                "thumb_160": "https:\/\/files.slack.com\/files-tmb\/T04MCQMEXQ9-F04MY7KSJ68-22e2022b4a\/________________________________2023-02-03_________________10.57.25_160.png?t=xoxe-4726837507825-4848681849303-4856614048758-e0b1f3d4cb371f92260edb0d9444d206",
                "thumb_720": "https:\/\/files.slack.com\/files-tmb\/T04MCQMEXQ9-F04MY7KSJ68-22e2022b4a\/________________________________2023-02-03_________________10.57.25_720.png?t=xoxe-4726837507825-4848681849303-4856614048758-e0b1f3d4cb371f92260edb0d9444d206",
                "thumb_720_w": 720,
                "thumb_720_h": 145,
                "thumb_800": "https:\/\/files.slack.com\/files-tmb\/T04MCQMEXQ9-F04MY7KSJ68-22e2022b4a\/________________________________2023-02-03_________________10.57.25_800.png?t=xoxe-4726837507825-4848681849303-4856614048758-e0b1f3d4cb371f92260edb0d9444d206",
                "thumb_800_w": 800,
                "thumb_800_h": 162,
                "thumb_960": "https:\/\/files.slack.com\/files-tmb\/T04MCQMEXQ9-F04MY7KSJ68-22e2022b4a\/________________________________2023-02-03_________________10.57.25_960.png?t=xoxe-4726837507825-4848681849303-4856614048758-e0b1f3d4cb371f92260edb0d9444d206",
                "thumb_960_w": 960,
                "thumb_960_h": 194,
                "thumb_1024": "https:\/\/files.slack.com\/files-tmb\/T04MCQMEXQ9-F04MY7KSJ68-22e2022b4a\/________________________________2023-02-03_________________10.57.25_1024.png?t=xoxe-4726837507825-4848681849303-4856614048758-e0b1f3d4cb371f92260edb0d9444d206",
                "thumb_1024_w": 1024,
                "thumb_1024_h": 207,
                "original_w": 1579,
                "original_h": 319,
                "thumb_tiny": "AwAJADDRYYBOTTRkjqfzp0n3aj7UAP5xnJ\/OjnGcn86ZRQA85A6\/rSgd8mo6lT7ooA\/\/2Q==",
                "permalink": "https:\/\/effai.slack.com\/files\/U04M2NY6U2Y\/F04MY7KSJ68\/________________________________2023-02-03_________________10.57.25.png",
                "permalink_public": "https:\/\/slack-files.com\/T04MCQMEXQ9-F04MY7KSJ68-ca84d09c74",
                "is_starred": false,
                "has_rich_preview": false,
                "file_access": "visible"
            },
            {
                "id": "F04MP5V4WES",
                "created": 1675389646,
                "timestamp": 1675389646,
                "name": "Image from iOS.jpg",
                "title": "Image from iOS",
                "mimetype": "image\/jpeg",
                "filetype": "jpg",
                "pretty_type": "JPEG",
                "user": "U04M2NY6U2Y",
                "user_team": "T04MCQMEXQ9",
                "editable": false,
                "size": 77483,
                "mode": "hosted",
                "is_external": false,
                "external_type": "",
                "is_public": true,
                "public_url_shared": false,
                "display_as_bot": false,
                "username": "",
                "subtype": "slack_image",
                "url_private": "https:\/\/files.slack.com\/files-pri\/T04MCQMEXQ9-F04MP5V4WES\/image_from_ios.jpg?t=xoxe-4726837507825-4848681849303-4856614048758-e0b1f3d4cb371f92260edb0d9444d206",
                "url_private_download": "https:\/\/files.slack.com\/files-pri\/T04MCQMEXQ9-F04MP5V4WES\/download\/image_from_ios.jpg?t=xoxe-4726837507825-4848681849303-4856614048758-e0b1f3d4cb371f92260edb0d9444d206",
                "media_display_type": "unknown",
                "thumb_64": "https:\/\/files.slack.com\/files-tmb\/T04MCQMEXQ9-F04MP5V4WES-d6e40fce19\/image_from_ios_64.jpg?t=xoxe-4726837507825-4848681849303-4856614048758-e0b1f3d4cb371f92260edb0d9444d206",
                "thumb_80": "https:\/\/files.slack.com\/files-tmb\/T04MCQMEXQ9-F04MP5V4WES-d6e40fce19\/image_from_ios_80.jpg?t=xoxe-4726837507825-4848681849303-4856614048758-e0b1f3d4cb371f92260edb0d9444d206",
                "thumb_360": "https:\/\/files.slack.com\/files-tmb\/T04MCQMEXQ9-F04MP5V4WES-d6e40fce19\/image_from_ios_360.jpg?t=xoxe-4726837507825-4848681849303-4856614048758-e0b1f3d4cb371f92260edb0d9444d206",
                "thumb_360_w": 360,
                "thumb_360_h": 71,
                "thumb_480": "https:\/\/files.slack.com\/files-tmb\/T04MCQMEXQ9-F04MP5V4WES-d6e40fce19\/image_from_ios_480.jpg?t=xoxe-4726837507825-4848681849303-4856614048758-e0b1f3d4cb371f92260edb0d9444d206",
                "thumb_480_w": 480,
                "thumb_480_h": 95,
                "thumb_160": "https:\/\/files.slack.com\/files-tmb\/T04MCQMEXQ9-F04MP5V4WES-d6e40fce19\/image_from_ios_160.jpg?t=xoxe-4726837507825-4848681849303-4856614048758-e0b1f3d4cb371f92260edb0d9444d206",
                "thumb_720": "https:\/\/files.slack.com\/files-tmb\/T04MCQMEXQ9-F04MP5V4WES-d6e40fce19\/image_from_ios_720.jpg?t=xoxe-4726837507825-4848681849303-4856614048758-e0b1f3d4cb371f92260edb0d9444d206",
                "thumb_720_w": 720,
                "thumb_720_h": 142,
                "thumb_800": "https:\/\/files.slack.com\/files-tmb\/T04MCQMEXQ9-F04MP5V4WES-d6e40fce19\/image_from_ios_800.jpg?t=xoxe-4726837507825-4848681849303-4856614048758-e0b1f3d4cb371f92260edb0d9444d206",
                "thumb_800_w": 800,
                "thumb_800_h": 158,
                "thumb_960": "https:\/\/files.slack.com\/files-tmb\/T04MCQMEXQ9-F04MP5V4WES-d6e40fce19\/image_from_ios_960.jpg?t=xoxe-4726837507825-4848681849303-4856614048758-e0b1f3d4cb371f92260edb0d9444d206",
                "thumb_960_w": 960,
                "thumb_960_h": 189,
                "thumb_1024": "https:\/\/files.slack.com\/files-tmb\/T04MCQMEXQ9-F04MP5V4WES-d6e40fce19\/image_from_ios_1024.jpg?t=xoxe-4726837507825-4848681849303-4856614048758-e0b1f3d4cb371f92260edb0d9444d206",
                "thumb_1024_w": 1024,
                "thumb_1024_h": 202,
                "original_w": 1741,
                "original_h": 343,
                "thumb_tiny": "AwAJADC\/3PWjI96D96mnqKAHce9HHvSdzQaAHcY6mnAY71F2oH3loA\/\/2Q==",
                "permalink": "https:\/\/effai.slack.com\/files\/U04M2NY6U2Y\/F04MP5V4WES\/image_from_ios.jpg",
                "permalink_public": "https:\/\/slack-files.com\/T04MCQMEXQ9-F04MP5V4WES-ffc2731962",
                "is_starred": false,
                "has_rich_preview": false,
                "file_access": "visible"
            }
        ],
        "upload": false,
        "user": "U04M2NY6U2Y",
        "ts": "1675389806.285319",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "xQbI",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "Appendix B.5 에 Faster Transformer Implementatio 을 통해 구현한 Inference Acceleration 결과가 있는데, Pytorch Implementation 에 비해 6-7배 빠르다는 결과도 보여주고 있더라구요. Optimization Target 을 보니 어찌보면 말씀하신 INT4 Weight 의 Dequantize 부분의 Overhead 를 "
                            },
                            {
                                "type": "text",
                                "text": "보강하기"
                            },
                            {
                                "type": "text",
                                "text": " "
                            },
                            {
                                "type": "text",
                                "text": "위한"
                            },
                            {
                                "type": "text",
                                "text": " "
                            },
                            {
                                "type": "text",
                                "text": "최적화라고도"
                            },
                            {
                                "type": "text",
                                "text": " "
                            },
                            {
                                "type": "text",
                                "text": "볼"
                            },
                            {
                                "type": "text",
                                "text": " "
                            },
                            {
                                "type": "text",
                                "text": "수"
                            },
                            {
                                "type": "text",
                                "text": " "
                            },
                            {
                                "type": "text",
                                "text": "있겠다는"
                            },
                            {
                                "type": "text",
                                "text": " "
                            },
                            {
                                "type": "text",
                                "text": "생각이"
                            },
                            {
                                "type": "text",
                                "text": " "
                            },
                            {
                                "type": "text",
                                "text": "듭니다"
                            },
                            {
                                "type": "text",
                                "text": "..\n\n"
                            },
                            {
                                "type": "text",
                                "text": "한"
                            },
                            {
                                "type": "text",
                                "text": " "
                            },
                            {
                                "type": "text",
                                "text": "가지"
                            },
                            {
                                "type": "text",
                                "text": " "
                            },
                            {
                                "type": "text",
                                "text": "궁금한"
                            },
                            {
                                "type": "text",
                                "text": " "
                            },
                            {
                                "type": "text",
                                "text": "점이"
                            },
                            {
                                "type": "text",
                                "text": " "
                            },
                            {
                                "type": "text",
                                "text": "있는데"
                            },
                            {
                                "type": "text",
                                "text": " Faster Trasnsformer "
                            },
                            {
                                "type": "text",
                                "text": "도"
                            },
                            {
                                "type": "text",
                                "text": " "
                            },
                            {
                                "type": "text",
                                "text": "실제로"
                            },
                            {
                                "type": "text",
                                "text": " "
                            },
                            {
                                "type": "text",
                                "text": "최적화를"
                            },
                            {
                                "type": "text",
                                "text": " "
                            },
                            {
                                "type": "text",
                                "text": "위해"
                            },
                            {
                                "type": "text",
                                "text": " "
                            },
                            {
                                "type": "text",
                                "text": "직접"
                            },
                            {
                                "type": "text",
                                "text": " "
                            },
                            {
                                "type": "text",
                                "text": "사용해보시는"
                            },
                            {
                                "type": "text",
                                "text": " "
                            },
                            {
                                "type": "text",
                                "text": "편인가요"
                            },
                            {
                                "type": "text",
                                "text": "?"
                            }
                        ]
                    }
                ]
            }
        ],
        "client_msg_id": "36ABBB4A-618D-428F-BBBE-0B826ECFF03F",
        "thread_ts": "1675336960.897349",
        "parent_user_id": "U04M2NY6U2Y"
    },
    {
        "client_msg_id": "8c5db6cc-c7e0-4d1d-8e6f-43dae48a44e9",
        "type": "message",
        "text": "네 FT가 pytorch 구현에 비해 훨씬 빠른건 당연하구요. 네이버가 재는 inference 성능은 당연히 FT입니다. 가끔 보면 huggingface를 그대로 쓰거나, 이상한 구현을 붙이는 경우가 있는데, 정당한 비교가 아닙니다.",
        "user": "U04M05UHX7U",
        "ts": "1675389908.068079",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "GVDf",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "네 FT가 pytorch 구현에 비해 훨씬 빠른건 당연하구요. 네이버가 재는 inference 성능은 당연히 FT입니다. 가끔 보면 huggingface를 그대로 쓰거나, 이상한 구현을 붙이는 경우가 있는데, 정당한 비교가 아닙니다."
                            }
                        ]
                    }
                ]
            }
        ],
        "team": "T04MCQMEXQ9",
        "user_team": "T04MCQMEXQ9",
        "source_team": "T04MCQMEXQ9",
        "user_profile": {
            "avatar_hash": "8160001f5214",
            "image_72": "https:\/\/avatars.slack-edge.com\/2023-01-29\/4699744492023_8160001f52148d13414d_72.jpg",
            "first_name": "권세중\/네이버클라우드",
            "real_name": "권세중\/네이버클라우드",
            "display_name": "권세중\/네이버클라우드",
            "team": "T04MCQMEXQ9",
            "name": "sejung.kwon",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "thread_ts": "1675336960.897349",
        "parent_user_id": "U04M2NY6U2Y"
    },
    {
        "client_msg_id": "325BDC0B-953A-4FEE-92D6-DA491FA80123",
        "type": "message",
        "text": "앞으로 FT 로 Inference 를 측정하는 방법도 잘 알아놔야겠군요.. 자세한 답변 정말 감사드립니다!! :man-bowing: ",
        "user": "U04M2NY6U2Y",
        "ts": "1675390280.712549",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "hCm6",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "앞으로"
                            },
                            {
                                "type": "text",
                                "text": " FT "
                            },
                            {
                                "type": "text",
                                "text": "로"
                            },
                            {
                                "type": "text",
                                "text": " Inference "
                            },
                            {
                                "type": "text",
                                "text": "를"
                            },
                            {
                                "type": "text",
                                "text": " "
                            },
                            {
                                "type": "text",
                                "text": "측정하는"
                            },
                            {
                                "type": "text",
                                "text": " "
                            },
                            {
                                "type": "text",
                                "text": "방법도"
                            },
                            {
                                "type": "text",
                                "text": " "
                            },
                            {
                                "type": "text",
                                "text": "잘"
                            },
                            {
                                "type": "text",
                                "text": " "
                            },
                            {
                                "type": "text",
                                "text": "알아놔야겠군요"
                            },
                            {
                                "type": "text",
                                "text": ".. "
                            },
                            {
                                "type": "text",
                                "text": "자세한"
                            },
                            {
                                "type": "text",
                                "text": " "
                            },
                            {
                                "type": "text",
                                "text": "답변"
                            },
                            {
                                "type": "text",
                                "text": " "
                            },
                            {
                                "type": "text",
                                "text": "정말"
                            },
                            {
                                "type": "text",
                                "text": " "
                            },
                            {
                                "type": "text",
                                "text": "감사드립니다"
                            },
                            {
                                "type": "text",
                                "text": "!! "
                            },
                            {
                                "type": "emoji",
                                "name": "man-bowing",
                                "unicode": "1f647-200d-2642-fe0f"
                            },
                            {
                                "type": "text",
                                "text": " "
                            }
                        ]
                    }
                ]
            }
        ],
        "team": "T04MCQMEXQ9",
        "user_team": "T04MCQMEXQ9",
        "source_team": "T04MCQMEXQ9",
        "user_profile": {
            "avatar_hash": "7026e9e9c4f8",
            "image_72": "https:\/\/avatars.slack-edge.com\/2023-01-29\/4699821662839_7026e9e9c4f8501ee814_72.png",
            "first_name": "김민수\/한양대",
            "real_name": "김민수\/한양대",
            "display_name": "김민수\/한양대",
            "team": "T04MCQMEXQ9",
            "name": "minsoo2333",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "thread_ts": "1675336960.897349",
        "parent_user_id": "U04M2NY6U2Y"
    },
    {
        "client_msg_id": "ab53a504-ad57-4749-8eb5-7ad7b7beb0c8",
        "type": "message",
        "text": "FT도 버전에 따라 많이 달라요 :slightly_smiling_face:",
        "user": "U04M05UHX7U",
        "ts": "1675390324.321049",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "EnBz",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "FT도 버전에 따라 많이 달라요 "
                            },
                            {
                                "type": "emoji",
                                "name": "slightly_smiling_face",
                                "unicode": "1f642"
                            }
                        ]
                    }
                ]
            }
        ],
        "team": "T04MCQMEXQ9",
        "user_team": "T04MCQMEXQ9",
        "source_team": "T04MCQMEXQ9",
        "user_profile": {
            "avatar_hash": "8160001f5214",
            "image_72": "https:\/\/avatars.slack-edge.com\/2023-01-29\/4699744492023_8160001f52148d13414d_72.jpg",
            "first_name": "권세중\/네이버클라우드",
            "real_name": "권세중\/네이버클라우드",
            "display_name": "권세중\/네이버클라우드",
            "team": "T04MCQMEXQ9",
            "name": "sejung.kwon",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "thread_ts": "1675336960.897349",
        "parent_user_id": "U04M2NY6U2Y",
        "reactions": [
            {
                "name": "astonished",
                "users": [
                    "U04M2NY6U2Y"
                ],
                "count": 1
            }
        ]
    }
]